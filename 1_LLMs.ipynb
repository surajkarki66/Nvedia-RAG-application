{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LLM\n",
        "----\n",
        "\n",
        "Across just about every domain, deploying massive deep learning models is a common yet challenging task. Today's models, such as Llama 2 (70B parameters) or ensemble models like Mixtral 7x8B, are products of advanced training methods, vast data resources, and powerful computing systems. Luckily for us, these models have already been trained and many use cases can already be achieved with off-the-shelf solutions. The real hurdle, however, lies in effectively hosting these models.\n",
        "\n",
        "**Deployment Scenarios for Large Models:**\n",
        "\n",
        "1. **High-End Datacenter Deployment:**\n",
        "> An uncompressed, unquantized model on a data center stack equipped with GPUs like NVIDIA's [A100](https://www.nvidia.com/en-us/data-center/a100/)/[H100](https://www.nvidia.com/en-us/data-center/h100/)/[H200](https://www.nvidia.com/en-us/data-center/h200/) to facilitate fast inference and experimentation.\n",
        "> - **Pros**: Ideal for scalable deployment and experimentation, this stack is ideal for either large training workflows or for supporting multiple users or models at the same time.  \n",
        "> - **Cons:** It is inefficient to allocate this resource for each user of your service unless the use cases involve model training/fine-tuning or interfacing with lower-level model components.\n",
        "\n",
        "2. **Modest Datacenter/Specialized Consumer Hardware Deployment:**\n",
        "> Quantized and further-optimized models can be run (one or two per instance) on more conservative datacenter GPUs such as [L40](https://www.nvidia.com/en-us/data-center/l40/)/[A30](https://www.nvidia.com/en-us/data-center/products/a30-gpu/)/[A10](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) or even on some modern consumer GPUs such as the higher-VRAM [RTX 40-series GPUs](https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/).\n",
        "> - **Pros:** This setup balances inference speed with manageable limitations for single-user applications. These sessions can also be deployed on a per-user basis to run one or two large models at a time with raw access to model internals (even if they need quantization).\n",
        "> - **Cons:** Deploying an instance for each user is still costly at scale, though it may be justifiable for some niche workloads. Alternatively, assuming that users can access these resources in their local environments is likely unreasonable.\n",
        "\n",
        "3. **Consumer Hardware Deployment:**\n",
        "> Though heavily limited in ability to propagate data through a neural network, most consumer hardware does have a graphical user interface (GUI), a web browser with internet access, some amount of memory (can safely assume at least 1 GB), and a decently-powerful CPU.\n",
        "> - **Cons:** Most hardware at the moment cannot run more than one local large model at a time in any configuration, and running even one model will require significant amounts of resource management and optimizing restrictions.\n",
        "> - **Pros:** This is a reasonable and inclusive starting assumption when considering what kinds of users your services should support.\n"
      ],
      "metadata": {
        "id": "9eBbv_8YqD9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Hosted Large Model Services\n",
        "\n",
        "**Black-Box Hosted Models:**\n",
        "> Services such as [**OpenAI**](https://openai.com/) offer APIs to interact with black-box models like GPT-4. These powerful, well-integrated services can provide simple interfaces to complex pipelines that automatically track memory, call additional models, and incorporate multimodal interfaces as necessary to simplify typical use scenarios. At the same time, they maintain operational opacity and often lack a straightforward path to self-hosting.\n",
        "> - **Pros:** Easy to use out-of-the-box with shallow barriers to entry for an average user.\n",
        "> - **Cons:** Black-box deployments suffer from potential privacy concerns, limited customization, and cost implications at scale.\n",
        "\n",
        "**Self-Hosted Models:**\n",
        "\n",
        "> Behind the scenes of just about all scaled model deployments is one or more giant models running in a data center with scalable resources and lightning-fast bandwidth at their disposal. Though necessary to deploy large models at scale and maintain strong control over the provided interfaces, these systems often require expertise to set up and generally do not work well for supporting non-developer workflows for only one individual at a time. Such systems are much better for supporting many simultaneous users, multiple models, and custom interfaces.\n",
        "> - **Pros:** They offer the capability to integrate custom datasets and APIs and are primarily designed to support numerous users concurrently.\n",
        "> - **Cons:** These setups demand technical expertise to set up and properly configure.\n",
        "\n",
        "To get the best of both worlds, we will utilize the [**NVIDIA NGC Service**](https://www.nvidia.com/en-us/gpu-cloud/). NGC offers a suite of developer tools for designing and deploying AI solutions. Central to our needs are the [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), which are pre-tuned and pre-optimized models designed for easy out-of-the-box scalable deployment (as-is or with further customization). Furthermore, NGC hosts accessible model endpoints for querying live foundation models in a [scalable DGX-accelerated compute environment](https://www.nvidia.com/en-us/data-center/dgx-platform/)."
      ],
      "metadata": {
        "id": "yy9dgoxjqz-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started With Hosted Inference\n",
        "**When deploying a model for scaled inference, the steps you generally need to take are as follows:**\n",
        "- Identify the models you would like users to access, and allocate resources to host them.\n",
        "- Figure out what kinds of controls you would like users to have, and expose ways for them to access it.\n",
        "- Create monitoring schemes to track/gate usage, and set up systems to scale and throttle as necessary.\n",
        "\n",
        "For this, we'll use the models deployed by NVIDIA, which are hosted as **LLM NIMs.** NIMs are microservices that are optimized to run AI workloads for scaled inference deployment. They work just fine for local inference and offer standardized APIs, but are primarily designed to work especially well in scaled environments. These particular models are deployed on NVIDIA DGX Cloud as shared functions and are advertised through an OpenAPI-style API gateway. Let's unpack what that means:\n",
        "\n",
        "**On The Cluster Side:** These microservices are hosted on a Kubernetes-backed platform that scales the load across a minimum and maximum number of DGX Nodes and are delivered behind a single function. In other words:\n",
        "- A large-language model is downloaded to and deployed on a **GPU-enabled compute node** (i.e. a powerful CPU and 4xH100-GPU environment which is physically-integrated in a DGX Pod).\n",
        "- On start, a selection of these compute nodes are kickstarted such that, whenever a user sends a request to the function, one of those nodes will receive the request.\n",
        "    - Kubernetes will route this traffic appropriately. If there is an idle compute node, it will receive the traffic. If all of them are working, the request will be queued up and a node will pick it up as soon as possible.\n",
        "    - In our case, these nodes will still pick up requests very fast since in-flight batching is enabled, meaning each node can take in up to 256 active requests at a time as-they-come before they get completely \"full\". (256 is a hyperparameter on deployment).\n",
        "- As load begins to increase, auto-scaling will kick in and more nodes will be kickstarted to avoid request handling delays.\n",
        "\n",
        "The following image shows an arbitrary function invocation with a custom (non-OpenAPI) API. This was the initial way in which the public endpoints were advertised, but is now an implementation detail.\n",
        "\n",
        "<!-- > <img style=\"max-width: 1000px;\" src=\"imgs/ai-playground-api.png\" /> -->\n",
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ckAIZoy7tvtK1uNqzA9eV5RlKMbVqs1-\" width=1000px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/ai-playground-api.png\" width=800px/>\n",
        "\n",
        "**On The Gateway Side:** To make this API more standard, an API gateway server is used to aggregate these functions behind a common API known as OpenAPI. This specification is subscribed to by many including OpenAI, so using the OpenAI client is a valid interface:\n",
        "\n",
        "<!-- > <img style=\"max-width: 800px;\" src=\"imgs/mixtral_api.png\" /> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/mixtral_api.png\" width=800px/>\n",
        "\n",
        "**On The User Side:** Incorporating these endpoints into your client, you can design integrations, pipelines, and user experiences that leverage these generative AI capabilities to endow your applications with reasoning and generative abilities. A popular example of such an application is [**OpenAI's ChatGPT**](https://chat.openai.com/), which is an orchestration of endpoints including GPT4, Dalle, and others. Though it may sometimes look like a single intelligent model, it is merely an aggregation of model endpoints with software engineering to help manage state and context control. This will be reinforced throughout the course, and by the end you should have an idea for how you could go about making a similar chat assistant for an arbitrary use-case.\n",
        "\n",
        "<!-- > <img style=\"max-width: 700px;\" src=\"imgs/openai_chat.png\" /> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/openai_chat.png\" width=700px/>\n"
      ],
      "metadata": {
        "id": "gLQGaXwgrzGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Trying Out The Foundation Model Endpoints\n",
        "\n"
      ],
      "metadata": {
        "id": "_5gLNflLtk9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "8w_zZm_nqslX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatNVIDIA Client Request\n",
        "In this experiment, we will want to do LLM orchestration with a framework called LangChain, so we'll need to go one layer of abstraction higher to a **Framework Connector**.\n",
        "\n",
        "The goal of a **connector** is to convert an arbitrary API from its native core into one that a target code-base would expect.\n",
        "\n",
        "Here, we'll want to take advantage of LangChain's thriving chain-centric ecosystem, but the raw `requests` API will not take us all the way there. Under the hood, every LangChain chat model that isn't hosted locally has to rely on such an API, but the developer-facing API is a much cleaner [`LLM` or `SimpleChatModel`-style interface](https://python.langchain.com/docs/modules/model_io/) with default parameters and some simple utility functions like `invoke` and `stream`.\n",
        "\n",
        "To start off our exploration into the LangChain interface, we will use the `ChatNVIDIA` connector to interface with our `chat/completions` endpoints. This model is part of the LangChain extended ecosystem and can be installed locally via `pip install langchain-nvidia-ai-endpoints`."
      ],
      "metadata": {
        "id": "zVnq6vHXvK1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZUE05XpvHGF",
        "outputId": "edf30763-410a-4482-b5ef-5ce3ba7acaa8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-nvidia-ai-endpoints in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from langchain-nvidia-ai-endpoints) (3.10.5)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.22 in /usr/local/lib/python3.10/dist-packages (from langchain-nvidia-ai-endpoints) (0.2.34)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain-nvidia-ai-endpoints) (10.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (0.1.104)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (2.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (3.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langchain-nvidia-ai-endpoints) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Using ChatNVIDIA\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "## NVIDIA_API_KEY pulled from environment\n",
        "llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
        "\n",
        "llm.invoke(\"Hello!, who are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kLbXbT-vQ5Y",
        "outputId": "9a58e211-521f-4e0c-9cb3-a58f6f7a71aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! I am a large language model trained by the Mistral AI team. I am designed to generate human-like text based on the input I receive. I do not have the ability to access personal data about individuals, perform external tasks, or maintain a stateful conception of myself beyond the current conversation. I am simply a tool for generating text based on the input I receive. How can I assist you today?', response_metadata={'role': 'assistant', 'content': 'Hello! I am a large language model trained by the Mistral AI team. I am designed to generate human-like text based on the input I receive. I do not have the ability to access personal data about individuals, perform external tasks, or maintain a stateful conception of myself beyond the current conversation. I am simply a tool for generating text based on the input I receive. How can I assist you today?', 'token_usage': {'prompt_tokens': 16, 'total_tokens': 101, 'completion_tokens': 85}, 'finish_reason': 'stop', 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1'}, id='run-215ba84f-079c-4571-832b-05cc95f996f4-0', role='assistant')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU7jRdZ5wqE4",
        "outputId": "4e313519-edcf-4674-9e6f-b73826f58ff9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='mistralai/mixtral-8x7b-instruct-v0.1')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = ChatNVIDIA.get_available_models()\n",
        "\n",
        "for model_card in model_list:\n",
        "    model_name = model_card.id\n",
        "\n",
        "    print(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCSbEYpNxvZw",
        "outputId": "a762483a-ce0e-4b2d-a7f4-778bcc0e4e89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta/llama2-70b\n",
            "writer/palmyra-med-70b-32k\n",
            "nvidia/llama3-chatqa-1.5-70b\n",
            "nvidia/usdcode-llama3-70b-instruct\n",
            "nvidia/neva-22b\n",
            "meta/llama-3.1-8b-instruct\n",
            "adept/fuyu-8b\n",
            "upstage/solar-10.7b-instruct\n",
            "meta/llama-3.1-70b-instruct\n",
            "google/deplot\n",
            "nvidia/nemotron-4-340b-instruct\n",
            "microsoft/phi-3-mini-4k-instruct\n",
            "meta/llama-3.1-405b-instruct\n",
            "seallms/seallm-7b-v2.5\n",
            "mediatek/breeze-7b-instruct\n",
            "ibm/granite-8b-code-instruct\n",
            "nv-mistralai/mistral-nemo-12b-instruct\n",
            "mistralai/codestral-22b-instruct-v0.1\n",
            "google/codegemma-7b\n",
            "meta/codellama-70b\n",
            "liuhaotian/llava-v1.6-34b\n",
            "google/gemma-2-27b-it\n",
            "meta/llama3-70b-instruct\n",
            "google/paligemma\n",
            "liuhaotian/llava-v1.6-mistral-7b\n",
            "mistralai/mistral-large\n",
            "deepseek-ai/deepseek-coder-6.7b-instruct\n",
            "ibm/granite-34b-code-instruct\n",
            "google/recurrentgemma-2b\n",
            "google/gemma-2-2b-it\n",
            "microsoft/phi-3-medium-4k-instruct\n",
            "mistralai/mixtral-8x22b-instruct-v0.1\n",
            "google/gemma-2-9b-it\n",
            "mistralai/mistral-7b-instruct-v0.2\n",
            "google/codegemma-1.1-7b\n",
            "mistralai/mistral-7b-instruct-v0.3\n",
            "writer/palmyra-med-70b\n",
            "mistralai/mixtral-8x7b-instruct-v0.1\n",
            "microsoft/phi-3-small-128k-instruct\n",
            "writer/palmyra-fin-70b-32k\n",
            "01-ai/yi-large\n",
            "nvidia/llama3-chatqa-1.5-8b\n",
            "microsoft/phi-3-small-8k-instruct\n",
            "aisingapore/sea-lion-7b-instruct\n",
            "google/gemma-7b\n",
            "microsoft/kosmos-2\n",
            "microsoft/phi-3-vision-128k-instruct\n",
            "google/gemma-2b\n",
            "microsoft/phi-3-mini-128k-instruct\n",
            "meta/llama3-8b-instruct\n",
            "mistralai/mamba-codestral-7b-v0.1\n",
            "microsoft/phi-3-medium-128k-instruct\n",
            "snowflake/arctic\n",
            "databricks/dbrx-instruct\n"
          ]
        }
      ]
    }
  ]
}