{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation with Vector Stores\n",
        "\n",
        "\n",
        "### **Environment Setup:**"
      ],
      "metadata": {
        "id": "xlyB77PqZ46q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain langchain-community langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SDbVhnPaTQT",
        "outputId": "7247f59a-3b52-45ab-b101-99f0a4ec359a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "z0FnPPcYc75P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ],
      "metadata": {
        "id": "sH1jfhHkaa5t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
      ],
      "metadata": {
        "id": "pr3NkDoHabv3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1: Summary of RAG Workflows\n",
        "\n",
        "This notebook will explore several paradigms and derive reference code to help you approach some of the most common retrieval-augmented workflows. Specifically, the following sections will be covered (with the differences highlighted):\n",
        "\n",
        "<br>\n",
        "\n",
        "> ***Vector Store Workflow for Conversational Exchanges:***\n",
        "- Generate semantic embedding for each new conversation.\n",
        "- Add the message body to a vector store for retrieval.\n",
        "- Query the vector store for relevant messages to fill in the LLM context.\n",
        "\n",
        "<br>\n",
        "\n",
        "> ***Modified Workflow for an Arbitrary Document:***\n",
        "- **Divide the document into chunks and process them into useful messages.**\n",
        "- Generate semantic embedding for each **new document chunk**.\n",
        "- Add the **chunk bodies** to a vector store for retrieval.\n",
        "- Query the vector store for relevant **chunks** to fill in the LLM context.\n",
        "    - ***Optional:* Modify/synthesize results for better LLM results.**\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Extended Workflow for a Directory of Arbitrary Documents:**\n",
        "- Divide **each document** into chunks and process them into useful messages.\n",
        "- Generate semantic embedding for each new document chunk.\n",
        "- Add the chunk bodies to **a scalable vector database for fast retrieval**.\n",
        "    - ***Optional*: Exploit hierarchical or metadata structures for larger systems.**\n",
        "- Query the **vector database** for relevant chunks to fill in the LLM context.\n",
        "    - *Optional:* Modify/synthesize results for better LLM results.\n",
        "\n",
        "<br>\n",
        "\n",
        "Some of the most important terminology surrounding RAG is covered in detail on the [**LlamaIndex Concepts page**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html), which itself is a great starting point for progressing towards the LlamaIndex loading and retrieving strategy. We highly recommend using it as a reference as you continue with this notebook and advise you to try out LlamaIndex after the course to consider the pros and cons firsthand!\n"
      ],
      "metadata": {
        "id": "CqgFdTjtai2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1cFbKbVvLLnFPs3yWCKIuzXkhBWh6nLQY\" width=1200px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
        ">\n",
        "> From [**Retrieval | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/docs/modules/data_connection/)"
      ],
      "metadata": {
        "id": "wDWCsl_rbuq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2:** RAG for Conversation History\n",
        "\n",
        "In our previous explorations, we delved into the capabilities of document embedding models and used them to embed, store, and compare semantic vector representations of text. Though we could motivate how to efficiently extend this into vector store land manually, the true beauty of working with a standard API is its strong incorporation with other frameworks that can already do the heavy lifting for us!\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "bSORmKnPbzzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1**: Getting A Conversation\n",
        "\n",
        "Consider a conversation crafted using Llama-13B between a chat agent and a blue bear named Beras. This dialogue, dense with details and potential diversions, provides a rich dataset for our study:\n"
      ],
      "metadata": {
        "id": "22wyYPFJb8db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
        "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
        "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
        "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
        "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
        "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
        "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
        "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
        "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
        "]"
      ],
      "metadata": {
        "id": "V1GJUTI4agXU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the manual embedding strategy from the previous notebook is still very viable, but we can also rest easy and let a **vector store** do all that work for us!\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "VuQz76vvcH1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2:** Constructing Our Vector Store Retriever\n",
        "\n",
        "To streamline similarity queries on our conversation, we can employ a vector store to help keep track of passages for us! **Vector Stores**, or vector storage systems, abstract away most of the low-level details of the embedding/comparison strategies and provide a simple interface to load and compare vectors.\n"
      ],
      "metadata": {
        "id": "iwrypWIwcNKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ZjwYbSZzsXK6ZP8O1-cY3BeRffV4oqzb\" width=1000px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
        ">\n",
        "> From [**Vector Stores | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
      ],
      "metadata": {
        "id": "whxqyCducT4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "In addition to simplifying the process from an API perspective, vector stores also implement connectors, integrations, and optimizations under the hood. In our case, we will start with the [**FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss), which integrates a LangChain-compatable Embedding model with the [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss) library to make the process fast and scalable on our local machine!\n",
        "\n",
        "**Specifically:**\n",
        "\n",
        "1. We can feed our conversation into [**a FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss) via the `from_texts` constructor. This will take our conversational data and the embedding model to create a searchable index over our discussion.\n",
        "2. This vector store can then be \"interpreted\" as a retriever, supporting the LangChain runnable API and returning documents retrieved via an input query.\n",
        "\n",
        "The following shows how you can construct a FAISS vector store and reinterpret it as a retriever using the LangChain `vectorstore` API:"
      ],
      "metadata": {
        "id": "r8TnQoSecZYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "## Streamlined from_texts FAISS vector store construction from text list\n",
        "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
        "retriever = convstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvxSS3PacF1k",
        "outputId": "f5df43ef-9626-4728-f1be-8c8dd9c18391"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 55.8 ms, sys: 9 ms, total: 64.8 ms\n",
            "Wall time: 1.23 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(retriever.invoke(\"Who are you?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "pIn__8rlcvx6",
        "outputId": "4056bba5-7b0e-4937-d357-c967560b0f4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
              "\u001b[32mrocky mountains?\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
              "\u001b[32mand their significance!'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001b[0m\n",
              "\u001b[32myou!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
              "\u001b[32macross North America'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "hU1yULZBdPTC",
        "outputId": "0e3552c1-dbd8-4d8e-f75c-e39bd694da13"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
              "\u001b[32macross North America'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
              "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
              "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
              "\u001b[32mrocky mountains?\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
              "\u001b[32mand their significance!'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, our retriever found a handful of semantically relevant documents from our query. You may notice that not all of the documents are useful or clear on their own. For example, a retrieval of *\"Beras\"* for *\"your name\"* may be problematic for the chatbot if provided out of context. Anticipating the potential problems and creating synergies between your LLM components can increase the likelihood of good RAG behavior, so keep an eye out for such pitfalls and opportunities.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "6Spi6XgldgDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3:** Incorporating Conversation Retrieval Into Our Chain\n",
        "\n",
        "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
        "- **A retriever is always retrieving context by default**.\n",
        "- **A generator is acting on the retrieved context**."
      ],
      "metadata": {
        "id": "NqalsUApdo8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "########################################################################\n",
        "## Utility Runnables/Methods\n",
        "def RPrint(preface=\"\"):\n",
        "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
        "    def print_and_return(x, preface):\n",
        "        if preface: print(preface, end=\"\")\n",
        "        pprint(x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name:\n",
        "            out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "## Optional; Reorders longer documents to center of output text\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
      ],
      "metadata": {
        "id": "urAMMzIQdaT2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Answer the question using only the context\"\n",
        "    \"\\n\\nRetrieved Context: {context}\"\n",
        "    \"\\n\\nUser Question: {question}\"\n",
        "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
        "        'question': (lambda x:x)\n",
        "    }\n",
        "    | context_prompt\n",
        "    | RPrint()\n",
        "    | instruct_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "pprint(chain.invoke(\"Where does Beras live?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "Fa06mXDzd0ao",
        "outputId": "bfb14f5f-f1f2-461c-8994-0e1ecbc40e12"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"Answer\u001b[0m\u001b[32m the question using only the context\\n\\nRetrieved Context: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32mHello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
              "\u001b[32mDocument\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I hope you get to visit them someday, Beras! It would be a great adventure for you!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Thank \u001b[0m\n",
              "\u001b[32myou for the suggestion! Ill definitely keep it in mind for the future.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wow, that \u001b[0m\n",
              "\u001b[32msounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote \u001b[0m\n",
              "\u001b[32mfrom Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research online \u001b[0m\n",
              "\u001b[32mor watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate there. I was\u001b[0m\n",
              "\u001b[32mjust curious, ya know!\\n\\n\\nUser Question: Where does Beras live?\\nAnswer the user conversationally. User is not \u001b[0m\n",
              "\u001b[32maware of context.\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Answer the question using only the context\\n\\nRetrieved Context: [Quote from Document] [User] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\\n[Quote from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Document] [Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you![Beras] Thank </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">you for the suggestion! Ill definitely keep it in mind for the future.\\n[Quote from Document] [Beras] Wow, that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\\n[Quote </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">from Document] [Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate there. I was</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">just curious, ya know!\\n\\n\\nUser Question: Where does Beras live?\\nAnswer the user conversationally. User is not </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">aware of context.\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mBased on the context provided, Beras lives in the Arctic. It seems like the Rocky Mountains are a fascinating place\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfor Beras to learn about, considering the vastly different climate from where they currently reside!\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on the context provided, Beras lives in the Arctic. It seems like the Rocky Mountains are a fascinating place</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for Beras to learn about, considering the vastly different climate from where they currently reside!</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a second to try out some more invocations and see how the new setup performs. Regardless of your model choice, the following questions should serve as interesting starting points."
      ],
      "metadata": {
        "id": "fENA5hZpeRy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "3-QdT7zYeK_1",
        "outputId": "e47b35ce-8c8f-4f3b-ed8f-10da52d1db7e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"Answer\u001b[0m\u001b[32m the question using only the context\\n\\nRetrieved Context: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\n",
              "\u001b[32mIn the meantime, you can learn more about the Rocky Mountains by doing some research online or watching \u001b[0m\n",
              "\u001b[32mdocumentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate there. I was just \u001b[0m\n",
              "\u001b[32mcurious, ya know!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about \u001b[0m\n",
              "\u001b[32mthe Rocky Mountains and their significance!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big \u001b[0m\n",
              "\u001b[32mblue bear! Can you please tell me about the rocky mountains?\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are\u001b[0m\n",
              "\u001b[32ma beautiful and majestic range of mountains that stretch across North America\\n\\n\\nUser Question: Where are the \u001b[0m\n",
              "\u001b[32mRocky Mountains?\\nAnswer the user conversationally. User is not aware of context.\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Answer the question using only the context\\n\\nRetrieved Context: [Quote from Document] [Agent]</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">curious, ya know!\\n[Quote from Document] [Agent] Absolutely! Lets continue the conversation and explore more about </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the Rocky Mountains and their significance!\\n[Quote from Document] [User]  Hello! My name is Beras, and I'm a big </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">blue bear! Can you please tell me about the rocky mountains?\\n[Quote from Document] [Agent] The Rocky Mountains are</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a beautiful and majestic range of mountains that stretch across North America\\n\\n\\nUser Question: Where are the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Rocky Mountains?\\nAnswer the user conversationally. User is not aware of context.\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwithout more context, I can't give you a precise location beyond that. However, you can find out more by doing some\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mresearch online or watching documentaries about them. I hope you enjoy learning about the Rocky Mountains!\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. Unfortunately, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">without more context, I can't give you a precise location beyond that. However, you can find out more by doing some</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">research online or watching documentaries about them. I hope you enjoy learning about the Rocky Mountains!</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "bZKyRdzYeVnL",
        "outputId": "7a79c388-c0eb-43c9-c23b-5de88a4437fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"Answer\u001b[0m\u001b[32m the question using only the context\\n\\nRetrieved Context: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\n",
              "\u001b[32mIn the meantime, you can learn more about the Rocky Mountains by doing some research online or watching \u001b[0m\n",
              "\u001b[32mdocumentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate there. I was just \u001b[0m\n",
              "\u001b[32mcurious, ya know!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about \u001b[0m\n",
              "\u001b[32mthe Rocky Mountains and their significance!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big \u001b[0m\n",
              "\u001b[32mblue bear! Can you please tell me about the rocky mountains?\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are\u001b[0m\n",
              "\u001b[32ma beautiful and majestic range of mountains that stretch across North America\\n\\n\\nUser Question: Where are the \u001b[0m\n",
              "\u001b[32mRocky Mountains? Are they close to California?\\nAnswer the user conversationally. User is not aware of context.\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Answer the question using only the context\\n\\nRetrieved Context: [Quote from Document] [Agent]</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">curious, ya know!\\n[Quote from Document] [Agent] Absolutely! Lets continue the conversation and explore more about </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the Rocky Mountains and their significance!\\n[Quote from Document] [User]  Hello! My name is Beras, and I'm a big </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">blue bear! Can you please tell me about the rocky mountains?\\n[Quote from Document] [Agent] The Rocky Mountains are</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a beautiful and majestic range of mountains that stretch across North America\\n\\n\\nUser Question: Where are the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Rocky Mountains? Are they close to California?\\nAnswer the user conversationally. User is not aware of context.\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. To answer your \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mquestion about their location, they do not directly border California. In fact, they run north to south, starting \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfrom western Canada and stretching all the way down through the states of Montana, Idaho, Wyoming, Colorado, and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minto New Mexico. So, while they're not close to California, they certainly are a significant part of the North \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mAmerican landscape. Is there anything else you'd like to know about them?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. To answer your </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question about their location, they do not directly border California. In fact, they run north to south, starting </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">from western Canada and stretching all the way down through the states of Montana, Idaho, Wyoming, Colorado, and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">into New Mexico. So, while they're not close to California, they certainly are a significant part of the North </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">American landscape. Is there anything else you'd like to know about them?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "3na8mXA4eaK9",
        "outputId": "ef0322cf-b8de-4752-b1c4-ea0fc637a84e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"Answer\u001b[0m\u001b[32m the question using only the context\\n\\nRetrieved Context: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\n",
              "\u001b[32mIn the meantime, you can learn more about the Rocky Mountains by doing some research online or watching \u001b[0m\n",
              "\u001b[32mdocumentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate there. I was just \u001b[0m\n",
              "\u001b[32mcurious, ya know!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of \u001b[0m\n",
              "\u001b[32mmountains that stretch across North America\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wow, that sounds amazing! Ive never been\u001b[0m\n",
              "\u001b[32mto the Rocky Mountains before, but Ive heard many great things about them.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My\u001b[0m\n",
              "\u001b[32mname is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\\n\\n\\nUser Question: How \u001b[0m\n",
              "\u001b[32mfar away is Beras from the Rocky Mountains?\\nAnswer the user conversationally. User is not aware of context.\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Answer the question using only the context\\n\\nRetrieved Context: [Quote from Document] [Agent]</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">curious, ya know!\\n[Quote from Document] [Agent] The Rocky Mountains are a beautiful and majestic range of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">mountains that stretch across North America\\n[Quote from Document] [Beras] Wow, that sounds amazing! Ive never been</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to the Rocky Mountains before, but Ive heard many great things about them.\\n[Quote from Document] [User]  Hello! My</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\\n\\n\\nUser Question: How </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">far away is Beras from the Rocky Mountains?\\nAnswer the user conversationally. User is not aware of context.\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mUnfortunately, the context doesn't provide information on Beras' exact location or distance from the Rocky \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mMountains. I can tell you, though, that the Rocky Mountains are a beautiful and majestic range of mountains \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mstretching across North America. If you'd like, I can search for more information about their geographical location\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand specific features.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Unfortunately, the context doesn't provide information on Beras' exact location or distance from the Rocky </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. I can tell you, though, that the Rocky Mountains are a beautiful and majestic range of mountains </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">stretching across North America. If you'd like, I can search for more information about their geographical location</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and specific features.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "You might notice some decent performance with this always-on retrieval node in the loop since the actual context being fed into the LLM remains relatively small. It's important to experiment with factors like embedding sizes, context limits, and model options to see what kinds of behavior you can expect and which efforts are worth taking to improve performance.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "3AhSwuATef63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4:** Automatic Conversation Storage\n",
        "\n",
        "Now that we see how our vector store memory unit should function, we can perform one last integration to allow our conversation to add new entries to our conversation: a runnable that calls the `add_texts` method for us to update the store state.\n"
      ],
      "metadata": {
        "id": "XbwPur3eemSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "## Reset knowledge base and define what it means to add more messages.\n",
        "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
        "    return d.get('output')\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Answer the question using only the context\"\n",
        "    \"\\n\\nRetrieved Context: {context}\"\n",
        "    \"\\n\\nUser Question: {input}\"\n",
        "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
        "    \"[Agent]\"\n",
        ")\n",
        "\n",
        "\n",
        "conv_chain = (\n",
        "    {\n",
        "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
        "        'input': (lambda x:x)\n",
        "    }\n",
        "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
        "    | partial(save_memory_and_get_output, vstore=convstore)\n",
        ")\n",
        "\n",
        "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "T405UBWGedpa",
        "outputId": "8ce12735-d6da-481d-9784-b5fac4c73bf5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mWhile I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's essential to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mremember that maintaining the pristine beauty of natural wonders like the Rockies is crucial. During our \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mexploration of the Rocky Mountains, we'll discuss their impressive features, wildlife, and how best to appreciate \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthem while minimizing our environmental impact. And who knows, perhaps on your future visit, you'll find a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdelightful spot to enjoy a sweet treat amidst the breathtaking scenery.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's essential to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">remember that maintaining the pristine beauty of natural wonders like the Rockies is crucial. During our </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">exploration of the Rocky Mountains, we'll discuss their impressive features, wildlife, and how best to appreciate </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">them while minimizing our environmental impact. And who knows, perhaps on your future visit, you'll find a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">delightful spot to enjoy a sweet treat amidst the breathtaking scenery.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mWhile we've been discussing the Rocky Mountains and the environmental considerations, I do recall a mention about a\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbeloved food of yours! You were quite enthusiastic about having some ice cream there. Given your excitement, it \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mseems safe to guess that ice cream might just be your favorite food. Would I be correct?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While we've been discussing the Rocky Mountains and the environmental considerations, I do recall a mention about a</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">beloved food of yours! You were quite enthusiastic about having some ice cream there. Given your excitement, it </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">seems safe to guess that ice cream might just be your favorite food. Would I be correct?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mAgent\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Oh, I see! I must have gotten mixed up earlier. It seems I incorrectly guessed that your favorite food was \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mice cream based on our conversation about enjoying a sweet treat in the Rockies. However, now that you've mentioned\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mit, honey does sound absolutely delightful. It's quite fascinating, isn't it, how a simple ingredient like honey \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcan be used in so many ways and dishes? I'm sure it adds a special touch to many of your favorite meals. Would you \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mlike to discuss more about honey or anything else you're interested in?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Agent] Oh, I see! I must have gotten mixed up earlier. It seems I incorrectly guessed that your favorite food was </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ice cream based on our conversation about enjoying a sweet treat in the Rockies. However, now that you've mentioned</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">it, honey does sound absolutely delightful. It's quite fascinating, isn't it, how a simple ingredient like honey </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can be used in so many ways and dishes? I'm sure it adds a special touch to many of your favorite meals. Would you </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like to discuss more about honey or anything else you're interested in?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mYes, I do! I apologize for the mix-up earlier. I now know that your favorite food is indeed honey. It must be quite\u001b[0m\n",
              "\u001b[1;38;2;118;185;0ma versatile ingredient to be used in so many ways and dishes! If you'd like, we can continue to discuss honey or \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mother topics that interest you.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Yes, I do! I apologize for the mix-up earlier. I now know that your favorite food is indeed honey. It must be quite</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a versatile ingredient to be used in so many ways and dishes! If you'd like, we can continue to discuss honey or </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">other topics that interest you.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the more automatic full-text or rule-based approaches to injecting context into the LLM, this approach ensures some amount of consolidation which can keep the context length from getting out of hand. It's still not a full-proof strategy on its own, but it's a stark improvement for unstructured conversations (and doesn't even require a strong instruction-tuned model to perform slot-filling)."
      ],
      "metadata": {
        "id": "vj1BPxY2fUY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **3:** RAG For Document Chunk Retrieval\n",
        "\n",
        "Given our prior exploration of document loading, the idea that data chunks can be embedded and searched through probably isn't surprising. With that said, it is definitely worth going over since applying RAG with documents is a double-edged sword; it may **seem** to work well out of the box but requires some extra care when optimizing it for truly reliable performance. It also provides an excellent opportunity to review some fundamental LCEL skills, so let's see what we can do!\n"
      ],
      "metadata": {
        "id": "O_ZFy_91feh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### **Task 1**: Loading And Chunking Your Documents\n",
        "\n",
        "A few simplifying assumptions and additional processing steps are included to help you improve your naive RAG performance:\n",
        "\n",
        "- Documents are cut off prior to the \"References\" section if one exists. This will keep our system from considering the citations and appendix sections, which tend to be long and distracting.\n",
        "\n",
        "- A chunk that lists the available documents is inserted to provide a high-level view of all available documents in a single chunk. If your pipeline does not provide metadata on each retrieval, this is a useful component and can even be listed among a list of higher-priority pieces if appropriate.\n",
        "\n",
        "- Additionally, the metadata entries are also inserted to provide general information. Ideally, there would also be some synthetic chunks that merge the metadata into interesting cross-document chunks.\n"
      ],
      "metadata": {
        "id": "aawWVUCuf3CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Loading Documents\")\n",
        "docs = [\n",
        "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
        "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
        "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
        "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
        "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
        "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
        "    ## Some longer papers\n",
        "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
        "    ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
        "    ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
        "    ArxivLoader(query=\"2408.11788\").load(), ## DreamFactory\n",
        "]\n",
        "\n",
        "## Cut the paper short if references is included.\n",
        "## This is a standard string in papers.\n",
        "for doc in docs:\n",
        "    content = json.dumps(doc[0].page_content)\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "\n",
        "## Split the documents and also filter out stubs (overly short chunks)\n",
        "print(\"Chunking Documents\")\n",
        "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
        "\n",
        "## Make some custom Chunks to give big-picture details\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    doc_string += \"\\n - \" + metadata.get('Title')\n",
        "    doc_metadata += [str(metadata)]\n",
        "\n",
        "extra_chunks = [doc_string] + doc_metadata\n",
        "\n",
        "## Printing out some summary information for reference\n",
        "pprint(doc_string, '\\n')\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    print(f\"Document {i}\")\n",
        "    print(f\" - # Chunks: {len(chunks)}\")\n",
        "    print(f\" - Metadata: \")\n",
        "    pprint(chunks[0].metadata)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "exiTz6dxfEBw",
        "outputId": "71f47d87-a32c-402c-9b75-b2d5abf29056"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Documents\n",
            "Chunking Documents\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
              "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - High-Resolution Image Synthesis with Latent Diffusion Models\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Learning Transferable Visual Models From Natural Language Supervision\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ReAct: Synergizing Reasoning and Acting in Language Models</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - High-Resolution Image Synthesis with Latent Diffusion Models</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Learning Transferable Visual Models From Natural Language Supervision</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0\n",
            " - # Chunks: 35\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
              "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001b[0m\n",
              "\u001b[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001b[0m\n",
              "\u001b[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001b[0m\n",
              "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001b[0m\n",
              "\u001b[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001b[0m\n",
              "\u001b[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001b[0m\n",
              "\u001b[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001b[0m\n",
              "\u001b[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001b[0m\n",
              "\u001b[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001b[0m\n",
              "\u001b[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001b[0m\n",
              "\u001b[32mconstituency parsing both with large and limited training data.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1\n",
            " - # Chunks: 45\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001b[0m\n",
              "\u001b[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001b[0m\n",
              "\u001b[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001b[0m\n",
              "\u001b[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001b[0m\n",
              "\u001b[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001b[0m\n",
              "\u001b[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001b[0m\n",
              "\u001b[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
              "\u001b[32mincluding\\npushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
              "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
              "\u001b[32mv2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 2\n",
            " - # Chunks: 46\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
              "\u001b[32mHeinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001b[0m\n",
              "\u001b[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001b[0m\n",
              "\u001b[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001b[0m\n",
              "\u001b[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001b[0m\n",
              "\u001b[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001b[0m\n",
              "\u001b[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001b[0m\n",
              "\u001b[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- \u001b[0m\n",
              "\u001b[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001b[0m\n",
              "\u001b[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
              "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
              "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
              "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
              "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
              "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
              "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 3\n",
            " - # Chunks: 40\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
              "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
              "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
              "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
              "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
              "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
              "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
              "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
              "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
              "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
              "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 4\n",
            " - # Chunks: 21\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
              "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, \u001b[0m\n",
              "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001b[0m\n",
              "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001b[0m\n",
              "\u001b[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for \u001b[0m\n",
              "\u001b[32mfaster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\nsequences of arbitrary length\u001b[0m\n",
              "\u001b[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001b[0m\n",
              "\u001b[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001b[0m\n",
              "\u001b[32munder the Apache 2.0 license.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 5\n",
            " - # Chunks: 44\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
              "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad \u001b[0m\n",
              "\u001b[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001b[0m\n",
              "\u001b[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001b[0m\n",
              "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001b[0m\n",
              "\u001b[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001b[0m\n",
              "\u001b[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001b[0m\n",
              "\u001b[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001b[0m\n",
              "\u001b[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001b[0m\n",
              "\u001b[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001b[0m\n",
              "\u001b[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001b[0m\n",
              "\u001b[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001b[0m\n",
              "\u001b[32mconversations with\\nhuman preferences are publicly available \u001b[0m\n",
              "\u001b[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 6\n",
            " - # Chunks: 123\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-03-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'ReAct: Synergizing Reasoning and Acting in Language Models'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'While large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have demonstrated impressive capabilities\\nacross tasks in \u001b[0m\n",
              "\u001b[32mlanguage understanding and interactive decision making, their\\nabilities for reasoning \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g. chain-of-thought \u001b[0m\n",
              "\u001b[32mprompting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and acting \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g.\\naction plan generation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have primarily been studied as separate topics. In \u001b[0m\n",
              "\u001b[32mthis\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an \u001b[0m\n",
              "\u001b[32minterleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, \u001b[0m\n",
              "\u001b[32mand update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, \u001b[0m\n",
              "\u001b[32msuch as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to \u001b[0m\n",
              "\u001b[32ma diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art \u001b[0m\n",
              "\u001b[32mbaselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting\u001b[0m\n",
              "\u001b[32mcomponents.\\nConcretely, on question answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotpotQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and fact verification \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFever\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nReAct overcomes issues of\u001b[0m\n",
              "\u001b[32mhallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia\u001b[0m\n",
              "\u001b[32mAPI, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without \u001b[0m\n",
              "\u001b[32mreasoning traces. On two interactive decision making\\nbenchmarks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mALFWorld and WebShop\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ReAct outperforms \u001b[0m\n",
              "\u001b[32mimitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being\u001b[0m\n",
              "\u001b[32mprompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-03-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ReAct: Synergizing Reasoning and Acting in Language Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 7\n",
            " - # Chunks: 52\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-04-13'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'High-Resolution Image Synthesis with Latent Diffusion Models'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'By decomposing the image formation process into a sequential application of\\ndenoising \u001b[0m\n",
              "\u001b[32mautoencoders, diffusion models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m achieve state-of-the-art\\nsynthesis results on image data and beyond. \u001b[0m\n",
              "\u001b[32mAdditionally, their formulation\\nallows for a guiding mechanism to control the image generation process \u001b[0m\n",
              "\u001b[32mwithout\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of \u001b[0m\n",
              "\u001b[32mpowerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To \u001b[0m\n",
              "\u001b[32menable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply \u001b[0m\n",
              "\u001b[32mthem in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion \u001b[0m\n",
              "\u001b[32mmodels on such a representation\\nallows for the first time to reach a near-optimal point between \u001b[0m\n",
              "\u001b[32mcomplexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention \u001b[0m\n",
              "\u001b[32mlayers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general \u001b[0m\n",
              "\u001b[32mconditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a \u001b[0m\n",
              "\u001b[32mconvolutional manner. Our latent diffusion models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLDMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m achieve\\na new state of the art for image inpainting and \u001b[0m\n",
              "\u001b[32mhighly competitive performance\\non various tasks, including unconditional image generation, semantic \u001b[0m\n",
              "\u001b[32mscene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to \u001b[0m\n",
              "\u001b[32mpixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-04-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'High-Resolution Image Synthesis with Latent Diffusion Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'By decomposing the image formation process into a sequential application of\\ndenoising </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">autoencoders, diffusion models (DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Additionally, their formulation\\nallows for a guiding mechanism to control the image generation process </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">without\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">powerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">enable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">them in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models on such a representation\\nallows for the first time to reach a near-optimal point between </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">complexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">layers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">convolutional manner. Our latent diffusion models (LDMs) achieve\\na new state of the art for image inpainting and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">highly competitive performance\\non various tasks, including unconditional image generation, semantic </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">scene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 8\n",
            " - # Chunks: 155\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-02-26'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Learning Transferable Visual Models From Natural Language Supervision'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish \u001b[0m\n",
              "\u001b[32mSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined \u001b[0m\n",
              "\u001b[32mobject categories. This restricted form of supervision limits\\ntheir generality and usability since additional \u001b[0m\n",
              "\u001b[32mlabeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a \u001b[0m\n",
              "\u001b[32mpromising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple \u001b[0m\n",
              "\u001b[32mpre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to \u001b[0m\n",
              "\u001b[32mlearn\\nSOTA image representations from scratch on a dataset of 400 million \u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage,\\ntext\u001b[0m\u001b[32m)\u001b[0m\u001b[32m pairs collected from the \u001b[0m\n",
              "\u001b[32minternet. After pre-training, natural language\\nis used to reference learned visual concepts \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor describe new ones\u001b[0m\u001b[32m)\u001b[0m\n",
              "\u001b[32menabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by \u001b[0m\n",
              "\u001b[32mbenchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action \u001b[0m\n",
              "\u001b[32mrecognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel \u001b[0m\n",
              "\u001b[32mtransfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need \u001b[0m\n",
              "\u001b[32mfor any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on \u001b[0m\n",
              "\u001b[32mImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release\u001b[0m\n",
              "\u001b[32mour code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-02-26'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Learning Transferable Visual Models From Natural Language Supervision'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">object categories. This restricted form of supervision limits\\ntheir generality and usability since additional </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">labeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">promising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">learn\\nSOTA image representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">internet. After pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones)</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 9\n",
            " - # Chunks: 41\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-08-21'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, Saad Ezzini'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Current video generation models excel at creating short, realistic clips, but\\nstruggle with \u001b[0m\n",
              "\u001b[32mlonger, multi-scene videos. We introduce \\\\texttt\u001b[0m\u001b[32m{\u001b[0m\u001b[32mDreamFactory\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\\nan LLM-based framework that tackles this \u001b[0m\n",
              "\u001b[32mchallenge. \\\\texttt\u001b[0m\u001b[32m{\u001b[0m\u001b[32mDreamFactory\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nleverages multi-agent collaboration principles and a Key Frames \u001b[0m\n",
              "\u001b[32mIteration\\nDesign Method to ensure consistency and style across long videos. It utilizes\\nChain of Thought \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCOT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to\u001b[0m\n",
              "\u001b[32maddress uncertainties inherent in large language\\nmodels. \\\\texttt\u001b[0m\u001b[32m{\u001b[0m\u001b[32mDreamFactory\u001b[0m\u001b[32m}\u001b[0m\u001b[32m generates long, stylistically \u001b[0m\n",
              "\u001b[32mcoherent, and\\ncomplex videos. Evaluating these long-form videos presents a challenge. We\\npropose novel metrics \u001b[0m\n",
              "\u001b[32msuch as Cross-Scene Face Distance Score and Cross-Scene\\nStyle Consistency Score. To further research in this area,\u001b[0m\n",
              "\u001b[32mwe contribute the\\nMulti-Scene Videos Dataset containing over 150 human-rated videos.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-08-21'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, Saad Ezzini'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Current video generation models excel at creating short, realistic clips, but\\nstruggle with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">longer, multi-scene videos. We introduce \\\\texttt{DreamFactory},\\nan LLM-based framework that tackles this </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">challenge. \\\\texttt{DreamFactory}\\nleverages multi-agent collaboration principles and a Key Frames </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Iteration\\nDesign Method to ensure consistency and style across long videos. It utilizes\\nChain of Thought (COT) to</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">address uncertainties inherent in large language\\nmodels. \\\\texttt{DreamFactory} generates long, stylistically </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">coherent, and\\ncomplex videos. Evaluating these long-form videos presents a challenge. We\\npropose novel metrics </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">such as Cross-Scene Face Distance Score and Cross-Scene\\nStyle Consistency Score. To further research in this area,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">we contribute the\\nMulti-Scene Videos Dataset containing over 150 human-rated videos.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### **Task 2**: Construct Your Document Vector Stores\n",
        "\n",
        "Now that we have all of the components, we can go ahead and create indices surrounding them:"
      ],
      "metadata": {
        "id": "9_Zd1Uxfhg3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(\"Constructing Vector Stores\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHnNTMcshIE0",
        "outputId": "d2ace371-68ae-40ef-f33c-75695333571a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing Vector Stores\n",
            "CPU times: user 1.84 s, sys: 113 ms, total: 1.96 s\n",
            "Wall time: 1min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0Eek0YDhrpY",
        "outputId": "8ddfd7e8-b5de-4590-9ea1-1734ed509c5f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Available Documents:\\n - Attention Is All You Need\\n - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\\n - Mistral 7B\\n - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\\n - ReAct: Synergizing Reasoning and Acting in Language Models\\n - High-Resolution Image Synthesis with Latent Diffusion Models\\n - Learning Transferable Visual Models From Natural Language Supervision\\n - DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework',\n",
              " \"{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks in an encoder-decoder configuration. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer, based\\\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to be\\\\nsuperior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\\\nEnglish-to-German translation task, improving over the existing best results,\\\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\\\ntranslation task, our model establishes a new single-model state-of-the-art\\\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\\\nof the training costs of the best models from the literature. We show that the\\\\nTransformer generalizes well to other tasks by applying it successfully to\\\\nEnglish constituency parsing both with large and limited training data.'}\",\n",
              " \"{'Published': '2019-05-24', 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova', 'Summary': 'We introduce a new language representation model called BERT, which stands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation models, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly conditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be fine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, such as question answering\\\\nand language inference, without substantial task-specific architecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains new\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point absolute improvement).'}\",\n",
              " \"{'Published': '2021-04-12', 'Title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks', 'Authors': 'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela', 'Summary': 'Large pre-trained language models have been shown to store factual knowledge\\\\nin their parameters, and achieve state-of-the-art results when fine-tuned on\\\\ndownstream NLP tasks. However, their ability to access and precisely manipulate\\\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\\\nperformance lags behind task-specific architectures. Additionally, providing\\\\nprovenance for their decisions and updating their world knowledge remain open\\\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\\\nexplicit non-parametric memory can overcome this issue, but have so far been\\\\nonly investigated for extractive downstream tasks. We explore a general-purpose\\\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\\\\ncombine pre-trained parametric and non-parametric memory for language\\\\ngeneration. We introduce RAG models where the parametric memory is a\\\\npre-trained seq2seq model and the non-parametric memory is a dense vector index\\\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\\\nformulations, one which conditions on the same retrieved passages across the\\\\nwhole generated sequence, the other can use different passages per token. We\\\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\\\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\\\\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\\\\nFor language generation tasks, we find that RAG models generate more specific,\\\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\\\nbaseline.'}\",\n",
              " '{\\'Published\\': \\'2022-05-01\\', \\'Title\\': \\'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\\', \\'Authors\\': \\'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz\\', \\'Summary\\': \\'Huge language models (LMs) have ushered in a new era for AI, serving as a\\\\ngateway to natural-language-based knowledge tasks. Although an essential\\\\nelement of modern AI, LMs are also inherently limited in a number of ways. We\\\\ndiscuss these limitations and how they can be avoided by adopting a systems\\\\napproach. Conceptualizing the challenge as one that involves knowledge and\\\\nreasoning in addition to linguistic processing, we define a flexible\\\\narchitecture with multiple neural models, complemented by discrete knowledge\\\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\\\nModular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\\\\nsome of the technical challenges in implementing it, and Jurassic-X, AI21 Labs\\\\\\'\\\\nMRKL system implementation.\\'}',\n",
              " \"{'Published': '2023-10-10', 'Title': 'Mistral 7B', 'Authors': 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed', 'Summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\\\ninference, coupled with sliding window attention (SWA) to effectively handle\\\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\\\nmodels are released under the Apache 2.0 license.'}\",\n",
              " \"{'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\\\njudges to evaluate these models on more open-ended questions. We examine the\\\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\\\nself-enhancement biases, as well as limited reasoning ability, and propose\\\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\\\nexplainable way to approximate human preferences, which are otherwise very\\\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\\\nhuman preferences are publicly available at\\\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}\",\n",
              " \"{'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\\\nacross tasks in language understanding and interactive decision making, their\\\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\\\naction plan generation) have primarily been studied as separate topics. In this\\\\npaper, we explore the use of LLMs to generate both reasoning traces and\\\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\\\nbetween the two: reasoning traces help the model induce, track, and update\\\\naction plans as well as handle exceptions, while actions allow it to interface\\\\nwith external sources, such as knowledge bases or environments, to gather\\\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\\\nlanguage and decision making tasks and demonstrate its effectiveness over\\\\nstate-of-the-art baselines, as well as improved human interpretability and\\\\ntrustworthiness over methods without reasoning or acting components.\\\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\\\nReAct overcomes issues of hallucination and error propagation prevalent in\\\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\\\ngenerates human-like task-solving trajectories that are more interpretable than\\\\nbaselines without reasoning traces. On two interactive decision making\\\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\\\nrespectively, while being prompted with only one or two in-context examples.\\\\nProject site with code: https://react-lm.github.io'}\",\n",
              " \"{'Published': '2022-04-13', 'Title': 'High-Resolution Image Synthesis with Latent Diffusion Models', 'Authors': 'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer', 'Summary': 'By decomposing the image formation process into a sequential application of\\\\ndenoising autoencoders, diffusion models (DMs) achieve state-of-the-art\\\\nsynthesis results on image data and beyond. Additionally, their formulation\\\\nallows for a guiding mechanism to control the image generation process without\\\\nretraining. However, since these models typically operate directly in pixel\\\\nspace, optimization of powerful DMs often consumes hundreds of GPU days and\\\\ninference is expensive due to sequential evaluations. To enable DM training on\\\\nlimited computational resources while retaining their quality and flexibility,\\\\nwe apply them in the latent space of powerful pretrained autoencoders. In\\\\ncontrast to previous work, training diffusion models on such a representation\\\\nallows for the first time to reach a near-optimal point between complexity\\\\nreduction and detail preservation, greatly boosting visual fidelity. By\\\\nintroducing cross-attention layers into the model architecture, we turn\\\\ndiffusion models into powerful and flexible generators for general conditioning\\\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\\\npossible in a convolutional manner. Our latent diffusion models (LDMs) achieve\\\\na new state of the art for image inpainting and highly competitive performance\\\\non various tasks, including unconditional image generation, semantic scene\\\\nsynthesis, and super-resolution, while significantly reducing computational\\\\nrequirements compared to pixel-based DMs. Code is available at\\\\nhttps://github.com/CompVis/latent-diffusion .'}\",\n",
              " \"{'Published': '2021-02-26', 'Title': 'Learning Transferable Visual Models From Natural Language Supervision', 'Authors': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever', 'Summary': 'State-of-the-art computer vision systems are trained to predict a fixed set\\\\nof predetermined object categories. This restricted form of supervision limits\\\\ntheir generality and usability since additional labeled data is needed to\\\\nspecify any other visual concept. Learning directly from raw text about images\\\\nis a promising alternative which leverages a much broader source of\\\\nsupervision. We demonstrate that the simple pre-training task of predicting\\\\nwhich caption goes with which image is an efficient and scalable way to learn\\\\nSOTA image representations from scratch on a dataset of 400 million (image,\\\\ntext) pairs collected from the internet. After pre-training, natural language\\\\nis used to reference learned visual concepts (or describe new ones) enabling\\\\nzero-shot transfer of the model to downstream tasks. We study the performance\\\\nof this approach by benchmarking on over 30 different existing computer vision\\\\ndatasets, spanning tasks such as OCR, action recognition in videos,\\\\ngeo-localization, and many types of fine-grained object classification. The\\\\nmodel transfers non-trivially to most tasks and is often competitive with a\\\\nfully supervised baseline without the need for any dataset specific training.\\\\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\\\\nzero-shot without needing to use any of the 1.28 million training examples it\\\\nwas trained on. We release our code and pre-trained model weights at\\\\nhttps://github.com/OpenAI/CLIP.'}\",\n",
              " \"{'Published': '2024-08-21', 'Title': 'DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework', 'Authors': 'Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend F. Bissyand, Saad Ezzini', 'Summary': 'Current video generation models excel at creating short, realistic clips, but\\\\nstruggle with longer, multi-scene videos. We introduce \\\\\\\\texttt{DreamFactory},\\\\nan LLM-based framework that tackles this challenge. \\\\\\\\texttt{DreamFactory}\\\\nleverages multi-agent collaboration principles and a Key Frames Iteration\\\\nDesign Method to ensure consistency and style across long videos. It utilizes\\\\nChain of Thought (COT) to address uncertainties inherent in large language\\\\nmodels. \\\\\\\\texttt{DreamFactory} generates long, stylistically coherent, and\\\\ncomplex videos. Evaluating these long-form videos presents a challenge. We\\\\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\\\\nStyle Consistency Score. To further research in this area, we contribute the\\\\nMulti-Scene Videos Dataset containing over 150 human-rated videos.'}\"]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vecstores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PGwLvwchttG",
        "outputId": "ec3d1c0b-1659-4781-8e21-aa23f84455cf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<langchain_community.vectorstores.faiss.FAISS at 0x7924410aee90>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x792440107f70>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x7924410adff0>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x7924410ad930>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x792440105c90>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x7924400f7880>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x7924682ef640>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x7924682ed060>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x7924410548b0>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x792441054100>,\n",
              " <langchain_community.vectorstores.faiss.FAISS at 0x7924410569e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    '''Useful utility for making an empty FAISS vectorstore'''\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vectorstores):\n",
        "    ## Initialize an empty FAISS Index and merge others into it\n",
        "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vectorstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
        "docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGzdvv7QiCb_",
        "outputId": "771769dd-51a9-4f3f-8765-d673a950178a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructed aggregate docstore with 613 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### **Task 3:** Implement Your RAG Chain\n",
        "\n",
        "Finally, all the puzzle pieces are in place to implement the RAG pipeline! As a review, we now have:\n",
        "\n",
        "- A way to construct a from-scratch vector store for conversational memory (and a way to initialize an empty one with `default_FAISS()`)\n",
        "\n",
        "- A vector store pre-loaded with useful document information from our `ArxivLoader` utility (stored in `docstore`).\n",
        "\n",
        "With the help of a couple more utilities, you're finally ready to integrate your chain! A few additional convenience utilities are provided (`doc2str` and the now-common `RPrint`) but are optional to use. Additionally, some starter prompts and structures are also defined.\n",
        "\n",
        "> **Given all of this:** Please implement the `retrieval_chain`."
      ],
      "metadata": {
        "id": "9VDD0B3DipJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import gradio as gr\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
        "\n",
        "\n",
        "convstore = default_FAISS()\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')\n",
        "\n",
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
        "\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}\n",
        "    ## Make sure to retrieve history & context from convstore & docstore, respectively.\n",
        "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
        "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
        "    | RPrint()\n",
        ")\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    ## First perform the retrieval based on the input message\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    ## Then, stream the results of the stream_chain\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        ## If you're using standard print, keep line from getting too long\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
        "\n",
        "\n",
        "## Start of Agent Event Loop\n",
        "test_question = \"Tell me a joke.\"  ## <- modify as desired\n",
        "\n",
        "## Before you launch your gradio interface, make sure your thing works\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zs5evCmqikXT",
        "outputId": "59aedbeb-aa96-42c5-b576-f36d0bf5796d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Tell me a joke.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'history'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m''\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'context'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m > clean knife 1 with \u001b[0m\n",
              "\u001b[32msinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a \u001b[0m\n",
              "\u001b[32mcellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a \u001b[0m\n",
              "\u001b[32mtomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn \u001b[0m\n",
              "\u001b[32mthe countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take\u001b[0m\n",
              "\u001b[32mknife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug \u001b[0m\n",
              "\u001b[32m2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn \u001b[0m\n",
              "\u001b[32mthe countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a \u001b[0m\n",
              "\u001b[32mknife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from \u001b[0m\n",
              "\u001b[32mcountertop 3\\nNothing happens.\\n> go to countertop 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from ReAct: Synergizing Reasoning and Acting in \u001b[0m\n",
              "\u001b[32mLanguage Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m 1.\\nYour task is to:\\nput a clean knife in countertop.\\n> think:\\nTo solve the task, I need to \u001b[0m\n",
              "\u001b[32mfind and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n> think:\\nFirst I need to \u001b[0m\n",
              "\u001b[32mfind a knife.\\nA knife is more likely to appear in cabinet\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, drawer \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, countertop \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, fridge \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
              "\u001b[32mgarbagecan \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, shelf \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nsinkbasin \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, stoveburner \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, toaster \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nI can check one by one, starting \u001b[0m\n",
              "\u001b[32mwith\\ncabinet 1.\\nOK.\\n> go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n> go to cabinet 2\\nThe cabinet 2 is \u001b[0m\n",
              "\u001b[32mclosed.\\n> go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n> go to cabinet 4\\nOn the cabinet 4, you \u001b[0m\n",
              "\u001b[32msee a mug 1.\\n> go to cabinet 5\\nThe cabinet 5 is closed.\\n> go to cabinet 6\\nThe cabinet 6 is closed.\\n> go to \u001b[0m\n",
              "\u001b[32mdrawer 1\\nThe drawer 1 is closed.\\n> go to drawer 2\\nThe drawer 2 is closed.\\n> go to drawer 3\\nThe drawer 3 is \u001b[0m\n",
              "\u001b[32mclosed.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon \u001b[0m\n",
              "\u001b[32m2.\\n> go to countertop 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m You are in the \u001b[0m\n",
              "\u001b[32mmiddle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a \u001b[0m\n",
              "\u001b[32mcabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2,\u001b[0m\n",
              "\u001b[32ma drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a \u001b[0m\n",
              "\u001b[32mstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean \u001b[0m\n",
              "\u001b[32mknife in countertop.\\n> think:\\nTo solve the task, I need to find and take a clean knife, then put it \u001b[0m\n",
              "\u001b[32min\\ncountertop.\\nOK.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you \u001b[0m\n",
              "\u001b[32msee a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to \u001b[0m\n",
              "\u001b[32mcountertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a \u001b[0m\n",
              "\u001b[32mpotato 3, and a statue 1.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> take knife 1 from countertop 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote \u001b[0m\n",
              "\u001b[32mfrom ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m > go to countertop 2\\nOn the countertop 2, you see\u001b[0m\n",
              "\u001b[32ma cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> take knife 1 from \u001b[0m\n",
              "\u001b[32mcountertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> go to countertop 1\\nOn the countertop 1, you see a \u001b[0m\n",
              "\u001b[32mlettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to \u001b[0m\n",
              "\u001b[32mcountertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a \u001b[0m\n",
              "\u001b[32mhouseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife \u001b[0m\n",
              "\u001b[32m2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, \u001b[0m\n",
              "\u001b[32ma glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go \u001b[0m\n",
              "\u001b[32mto countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife\u001b[0m\n",
              "\u001b[32m1 with sinkbasin 1\\nNothing happens.\\n'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a joke.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from ReAct: Synergizing Reasoning and Acting in Language Models] &gt; clean knife 1 with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sinkbasin 1\\nNothing happens.\\n&gt; go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tomato\\n2, a tomato 1, and a vase 2.\\n&gt; take knife 2 from countertop 3\\nNothing happens.\\n&gt; go to countertop 2\\nOn </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n&gt; take</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knife 1 from countertop 2\\nNothing happens.\\n&gt; go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2, a peppershaker 1, and a spoon 2.\\n&gt; clean knife 1 with sinkbasin 1\\nNothing happens.\\n&gt; go to countertop 3\\nOn </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n&gt; take knife 2 from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">countertop 3\\nNothing happens.\\n&gt; go to countertop 2\\n[Quote from ReAct: Synergizing Reasoning and Acting in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Language Models] 1.\\nYour task is to:\\nput a clean knife in countertop.\\n&gt; think:\\nTo solve the task, I need to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">find and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n&gt; think:\\nFirst I need to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">find a knife.\\nA knife is more likely to appear in cabinet\\n(1-6), drawer (1-3), countertop (1-3), fridge (1), </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">garbagecan (1), shelf (1-3),\\nsinkbasin (1), stoveburner (1-4), toaster (1).\\nI can check one by one, starting </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with\\ncabinet 1.\\nOK.\\n&gt; go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n&gt; go to cabinet 2\\nThe cabinet 2 is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">closed.\\n&gt; go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n&gt; go to cabinet 4\\nOn the cabinet 4, you </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">see a mug 1.\\n&gt; go to cabinet 5\\nThe cabinet 5 is closed.\\n&gt; go to cabinet 6\\nThe cabinet 6 is closed.\\n&gt; go to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">drawer 1\\nThe drawer 1 is closed.\\n&gt; go to drawer 2\\nThe drawer 2 is closed.\\n&gt; go to drawer 3\\nThe drawer 3 is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">closed.\\n&gt; go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2.\\n&gt; go to countertop 2\\n[Quote from ReAct: Synergizing Reasoning and Acting in Language Models] You are in the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knife in countertop.\\n&gt; think:\\nTo solve the task, I need to find and take a clean knife, then put it </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">in\\ncountertop.\\nOK.\\n&gt; think:\\nI need to find a clean knife.\\nOK.\\n&gt; go to countertop 1\\nOn the countertop 1, you </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n&gt; think:\\nI need to find a clean knife.\\nOK.\\n&gt; go to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">potato 3, and a statue 1.\\n&gt; think:\\nI need to find a clean knife.\\nOK.\\n&gt; take knife 1 from countertop 2\\n[Quote </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">from ReAct: Synergizing Reasoning and Acting in Language Models] &gt; go to countertop 2\\nOn the countertop 2, you see</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n&gt; take knife 1 from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n&gt; go to countertop 1\\nOn the countertop 1, you see a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n&gt; clean knife 1 with sinkbasin 1\\nNothing happens.\\n&gt; go to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n&gt; take knife </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2 from countertop 3\\nNothing happens.\\n&gt; go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n&gt; take knife 1 from countertop 2\\nNothing happens.\\n&gt; go </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n&gt; clean knife</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">1 with sinkbasin 1\\nNothing happens.\\n'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
              "\u001b[32mjust asked: Tell me a joke.\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation \u001b[0m\n",
              "\u001b[32mHistory Retrieval:\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from ReAct: Synergizing Reasoning and Acting in Language \u001b[0m\n",
              "\u001b[32mModels\u001b[0m\u001b[32m]\u001b[0m\u001b[32m > clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to countertop 3\\nOn the countertop 3, you see a \u001b[0m\n",
              "\u001b[32mbread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a \u001b[0m\n",
              "\u001b[32mstatue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife 2 from countertop 3\\nNothing happens.\\n>\u001b[0m\n",
              "\u001b[32mgo to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, \u001b[0m\n",
              "\u001b[32mand a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go to countertop 1\\nOn the countertop 1, you\u001b[0m\n",
              "\u001b[32msee a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> \u001b[0m\n",
              "\u001b[32mgo to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork\u001b[0m\n",
              "\u001b[32m2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take \u001b[0m\n",
              "\u001b[32mknife 2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from ReAct: Synergizing Reasoning and \u001b[0m\n",
              "\u001b[32mActing in Language Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m 1.\\nYour task is to:\\nput a clean knife in countertop.\\n> think:\\nTo solve the task, I \u001b[0m\n",
              "\u001b[32mneed to find and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n> think:\\nFirst I \u001b[0m\n",
              "\u001b[32mneed to find a knife.\\nA knife is more likely to appear in cabinet\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, drawer \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, countertop \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, fridge \u001b[0m\n",
              "\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, garbagecan \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, shelf \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nsinkbasin \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, stoveburner \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1-4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, toaster \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nI can check one by one, starting\u001b[0m\n",
              "\u001b[32mwith\\ncabinet 1.\\nOK.\\n> go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n> go to cabinet 2\\nThe cabinet 2 is \u001b[0m\n",
              "\u001b[32mclosed.\\n> go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n> go to cabinet 4\\nOn the cabinet 4, you \u001b[0m\n",
              "\u001b[32msee a mug 1.\\n> go to cabinet 5\\nThe cabinet 5 is closed.\\n> go to cabinet 6\\nThe cabinet 6 is closed.\\n> go to \u001b[0m\n",
              "\u001b[32mdrawer 1\\nThe drawer 1 is closed.\\n> go to drawer 2\\nThe drawer 2 is closed.\\n> go to drawer 3\\nThe drawer 3 is \u001b[0m\n",
              "\u001b[32mclosed.\\n> go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon \u001b[0m\n",
              "\u001b[32m2.\\n> go to countertop 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m You are in the \u001b[0m\n",
              "\u001b[32mmiddle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a \u001b[0m\n",
              "\u001b[32mcabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2,\u001b[0m\n",
              "\u001b[32ma drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a \u001b[0m\n",
              "\u001b[32mstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean \u001b[0m\n",
              "\u001b[32mknife in countertop.\\n> think:\\nTo solve the task, I need to find and take a clean knife, then put it \u001b[0m\n",
              "\u001b[32min\\ncountertop.\\nOK.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to countertop 1\\nOn the countertop 1, you \u001b[0m\n",
              "\u001b[32msee a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> go to \u001b[0m\n",
              "\u001b[32mcountertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a \u001b[0m\n",
              "\u001b[32mpotato 3, and a statue 1.\\n> think:\\nI need to find a clean knife.\\nOK.\\n> take knife 1 from countertop 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote \u001b[0m\n",
              "\u001b[32mfrom ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\u001b[32m]\u001b[0m\u001b[32m > go to countertop 2\\nOn the countertop 2, you see\u001b[0m\n",
              "\u001b[32ma cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n> take knife 1 from \u001b[0m\n",
              "\u001b[32mcountertop 2\\nYou pick up the knife 1 from the countertop 2.\\n> go to countertop 1\\nOn the countertop 1, you see a \u001b[0m\n",
              "\u001b[32mlettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife 1 with sinkbasin 1\\nNothing happens.\\n> go to \u001b[0m\n",
              "\u001b[32mcountertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a \u001b[0m\n",
              "\u001b[32mhouseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n> take knife \u001b[0m\n",
              "\u001b[32m2 from countertop 3\\nNothing happens.\\n> go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, \u001b[0m\n",
              "\u001b[32ma glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n> take knife 1 from countertop 2\\nNothing happens.\\n> go \u001b[0m\n",
              "\u001b[32mto countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n> clean knife\u001b[0m\n",
              "\u001b[32m1 with sinkbasin 1\\nNothing happens.\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources that are used. Make your \u001b[0m\n",
              "\u001b[32mresponse conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me a joke.'\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me a joke.\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from ReAct: Synergizing Reasoning and Acting in Language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Models] &gt; clean knife 1 with sinkbasin 1\\nNothing happens.\\n&gt; go to countertop 3\\nOn the countertop 3, you see a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n&gt; take knife 2 from countertop 3\\nNothing happens.\\n&gt;</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\\npotato 3, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and a statue 1.\\n&gt; take knife 1 from countertop 2\\nNothing happens.\\n&gt; go to countertop 1\\nOn the countertop 1, you</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n&gt; clean knife 1 with sinkbasin 1\\nNothing happens.\\n&gt; </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">go to countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n&gt; take </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knife 2 from countertop 3\\nNothing happens.\\n&gt; go to countertop 2\\n[Quote from ReAct: Synergizing Reasoning and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Acting in Language Models] 1.\\nYour task is to:\\nput a clean knife in countertop.\\n&gt; think:\\nTo solve the task, I </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">need to find and take a knife, then clean it with\\nsinkbasin, then put it in countertop.\\nOK.\\n&gt; think:\\nFirst I </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">need to find a knife.\\nA knife is more likely to appear in cabinet\\n(1-6), drawer (1-3), countertop (1-3), fridge </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">(1), garbagecan (1), shelf (1-3),\\nsinkbasin (1), stoveburner (1-4), toaster (1).\\nI can check one by one, starting</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with\\ncabinet 1.\\nOK.\\n&gt; go to cabinet 1\\nOn the cabinet 1, you see a bowl 1.\\n&gt; go to cabinet 2\\nThe cabinet 2 is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">closed.\\n&gt; go to cabinet 3\\nOn the cabinet 3, you see a glassbottle 1.\\n&gt; go to cabinet 4\\nOn the cabinet 4, you </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">see a mug 1.\\n&gt; go to cabinet 5\\nThe cabinet 5 is closed.\\n&gt; go to cabinet 6\\nThe cabinet 6 is closed.\\n&gt; go to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">drawer 1\\nThe drawer 1 is closed.\\n&gt; go to drawer 2\\nThe drawer 2 is closed.\\n&gt; go to drawer 3\\nThe drawer 3 is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">closed.\\n&gt; go to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2.\\n&gt; go to countertop 2\\n[Quote from ReAct: Synergizing Reasoning and Acting in Language Models] You are in the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">middle of a room.\\nLooking quickly around you, you see a cabinet 6, a\\ncabinet 5, a cabinet 4, a cabinet 3, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">cabinet 2, a cabinet 1, a coffeemachine 1, a\\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a drawer 1, a\\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\\n1, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\\n1.\\nYour task is to:\\nput a clean </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knife in countertop.\\n&gt; think:\\nTo solve the task, I need to find and take a clean knife, then put it </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">in\\ncountertop.\\nOK.\\n&gt; think:\\nI need to find a clean knife.\\nOK.\\n&gt; go to countertop 1\\nOn the countertop 1, you </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n&gt; think:\\nI need to find a clean knife.\\nOK.\\n&gt; go to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">potato 3, and a statue 1.\\n&gt; think:\\nI need to find a clean knife.\\nOK.\\n&gt; take knife 1 from countertop 2\\n[Quote </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">from ReAct: Synergizing Reasoning and Acting in Language Models] &gt; go to countertop 2\\nOn the countertop 2, you see</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\\nplate 2, a potato 3, and a statue 1.\\n&gt; take knife 1 from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">countertop 2\\nYou pick up the knife 1 from the countertop 2.\\n&gt; go to countertop 1\\nOn the countertop 1, you see a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n&gt; clean knife 1 with sinkbasin 1\\nNothing happens.\\n&gt; go to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">countertop 3\\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\\n1, a fork 2, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\\n2, a tomato 1, and a vase 2.\\n&gt; take knife </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2 from countertop 3\\nNothing happens.\\n&gt; go to countertop 2\\nOn the countertop 2, you see a cup 1, a dishsponge 1, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a glassbottle 3, a plate 2, a\\npotato 3, and a statue 1.\\n&gt; take knife 1 from countertop 2\\nNothing happens.\\n&gt; go </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to countertop 1\\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\\n&gt; clean knife</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">1 with sinkbasin 1\\nNothing happens.\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">response conversational.)'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a joke.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm here to help answer questions about documents, but I can certainly share a joke with you! Here it is:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "(I didn't find any relevant information in the provided document for this joke, it's just a classic one I thought I'd share.)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 4:** Interact With Your Gradio Chatbot"
      ],
      "metadata": {
        "id": "FDGQgzRfjuGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "653Wz2qvjiIN",
        "outputId": "d0b4b2e4-16d4-4c09-b47f-b144e439b4dd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://3d245c691af92680ca.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3d245c691af92680ca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'who is the main author of Mistral 7B paper?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'history'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with Tell me a joke.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent \u001b[0m\n",
              "\u001b[32mpreviously responded with I'm here to help answer questions about documents, but I can certainly share a joke with \u001b[0m\n",
              "\u001b[32myou! Here it is:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mI didn't find any \u001b[0m\n",
              "\u001b[32mrelevant information in the provided document for this joke, it's just a classic one I thought I'd share.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'context'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Mistral 7B\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Mistral 7B outperforms the previous best 13B model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLlama 2, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m across\u001b[0m\n",
              "\u001b[32mall tested\\\\nbenchmarks, and surpasses the best 34B model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLaMa 34B, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m25\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in mathematics and code\\\\ngeneration. \u001b[0m\n",
              "\u001b[32mFurthermore, Mistral 7B approaches the coding performance of Code-Llama 7B \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\\nwithout sacrificing performance \u001b[0m\n",
              "\u001b[32mon non-code related benchmarks.\\\\nMistral 7B leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and sliding window \u001b[0m\n",
              "\u001b[32mattention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6, 3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\\\nGQA significantly accelerates the inference speed, and also reduces the memory \u001b[0m\n",
              "\u001b[32mrequirement during\\\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for \u001b[0m\n",
              "\u001b[32mreal-time\\\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a \u001b[0m\n",
              "\u001b[32mreduced\\\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention \u001b[0m\n",
              "\u001b[32mmechanisms\\\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.\\\\narXiv:2310.06825v1\u001b[0m\n",
              "\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  10 Oct 2023\\\\nMistral 7B is released under the Apache 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'Published\\': \u001b[0m\n",
              "\u001b[32m\\'2023-10-10\\', \\'Title\\': \\'Mistral 7B\\', \\'Authors\\': \\'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, \u001b[0m\n",
              "\u001b[32mChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, \u001b[0m\n",
              "\u001b[32mLucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,\u001b[0m\n",
              "\u001b[32mTimoth√©e Lacroix, William El Sayed\\', \\'Summary\\': \\'We introduce Mistral 7B v0.1, a 7-billion-parameter language \u001b[0m\n",
              "\u001b[32mmodel engineered\\\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\\\nacross all \u001b[0m\n",
              "\u001b[32mevaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\\\ncode generation. Our model leverages \u001b[0m\n",
              "\u001b[32mgrouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for faster\\\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively \u001b[0m\n",
              "\u001b[32mhandle\\\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\\\nmodel fine-tuned to \u001b[0m\n",
              "\u001b[32mfollow instructions, Mistral 7B -- Instruct, that surpasses\\\\nthe Llama 2 13B -- Chat model both on human and \u001b[0m\n",
              "\u001b[32mautomated benchmarks. Our\\\\nmodels are released under the Apache 2.0 license.\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Mistral 7B\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .\\\\nNo \u001b[0m\n",
              "\u001b[32mproprietary data or training tricks were utilized:\\\\nMistral 7B \\\\u2013 Instruct model is a simple \u001b[0m\n",
              "\u001b[32mand\\\\npreliminary demonstration that the base model can\\\\neasily be fine-tuned to achieve good performance.\\\\nIn \u001b[0m\n",
              "\u001b[32mTable 3, we observe that the resulting model,\\\\nMistral 7B \\\\u2013 Instruct, exhibits superior perfor-\\\\nmance \u001b[0m\n",
              "\u001b[32mcompared to all 7B models on MT-Bench,\\\\nand is comparable to 13B \\\\u2013 Chat models. An\\\\nindependent human \u001b[0m\n",
              "\u001b[32mevaluation was conducted on\\\\nhttps://llmboxing.com/leaderboard.\\\\nIn this evaluation, participants were provided \u001b[0m\n",
              "\u001b[32mwith a set of questions along with anonymous responses\\\\nfrom two models and were asked to select their preferred \u001b[0m\n",
              "\u001b[32mresponse, as illustrated in Figure 6. As of\\\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred \u001b[0m\n",
              "\u001b[32m5020 times, compared to 4143\\\\ntimes for Llama 2 13B.\\\\n4\\\\nFigure 5: Results on MMLU, commonsense reasoning, world\u001b[0m\n",
              "\u001b[32mknowledge and reading comprehension for\\\\nMistral 7B and Llama 2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7B/13B/70B\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Mistral 7B\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \"Mistral \u001b[0m\n",
              "\u001b[32m7B\\\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\\\nDevendra Singh Chaplot, Diego de las \u001b[0m\n",
              "\u001b[32mCasas, Florian Bressand, Gianna Lengyel,\\\\nGuillaume Lample, Lucile Saulnier, L\\\\u00e9lio Renard Lavaud, Marie-Anne\u001b[0m\n",
              "\u001b[32mLachaux,\\\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\\\\u00e9e Lacroix,\\\\nWilliam El \u001b[0m\n",
              "\u001b[32mSayed\\\\nAbstract\\\\nWe introduce Mistral 7B, a 7\\\\u2013billion-parameter language model engineered for\\\\nsuperior \u001b[0m\n",
              "\u001b[32mperformance and efficiency. Mistral 7B outperforms the best open 13B\\\\nmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLlama 2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m across all evaluated \u001b[0m\n",
              "\u001b[32mbenchmarks, and the best released 34B\\\\nmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLlama 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in reasoning, mathematics, and code generation. Our \u001b[0m\n",
              "\u001b[32mmodel\\\\nleverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for faster inference, coupled with sliding\\\\nwindow attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\n",
              "\u001b[32mto effectively handle sequences of arbitrary length with a\\\\nreduced inference cost\\n'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'who is the main author of Mistral 7B paper?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Quote from Document] User previously responded with Tell me a joke.\\n[Quote from Document] Agent </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">previously responded with I'm here to help answer questions about documents, but I can certainly share a joke with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">you! Here it is:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\n(I didn't find any </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">relevant information in the provided document for this joke, it's just a classic one I thought I'd share.)\\n\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from Mistral 7B] . Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">all tested\\\\nbenchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code\\\\ngeneration. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20],\\\\nwithout sacrificing performance </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">on non-code related benchmarks.\\\\nMistral 7B leverages grouped-query attention (GQA) [1], and sliding window </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">attention (SWA) [6, 3].\\\\nGQA significantly accelerates the inference speed, and also reduces the memory </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">requirement during\\\\ndecoding, allowing for higher batch sizes hence higher throughput, a crucial factor for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">real-time\\\\napplications. In addition, SWA is designed to handle longer sequences more effectively at a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reduced\\\\ncomputational cost, thereby alleviating a common limitation in LLMs. These attention </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">mechanisms\\\\ncollectively contribute to the enhanced performance and efficiency of Mistral 7B.\\\\narXiv:2310.06825v1</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">[cs.CL]  10 Oct 2023\\\\nMistral 7B is released under the Apache 2\\n[Quote from Document] {\\'Published\\': </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\'2023-10-10\\', \\'Title\\': \\'Mistral 7B\\', \\'Authors\\': \\'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Timoth√©e Lacroix, William El Sayed\\', \\'Summary\\': \\'We introduce Mistral 7B v0.1, a 7-billion-parameter language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model engineered\\\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\\\nacross all </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\\\ncode generation. Our model leverages </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">grouped-query attention (GQA) for faster\\\\ninference, coupled with sliding window attention (SWA) to effectively </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">handle\\\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\\\nmodel fine-tuned to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">follow instructions, Mistral 7B -- Instruct, that surpasses\\\\nthe Llama 2 13B -- Chat model both on human and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">automated benchmarks. Our\\\\nmodels are released under the Apache 2.0 license.\\'}\\n[Quote from Mistral 7B] .\\\\nNo </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">proprietary data or training tricks were utilized:\\\\nMistral 7B \\\\u2013 Instruct model is a simple </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\npreliminary demonstration that the base model can\\\\neasily be fine-tuned to achieve good performance.\\\\nIn </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Table 3, we observe that the resulting model,\\\\nMistral 7B \\\\u2013 Instruct, exhibits superior perfor-\\\\nmance </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">compared to all 7B models on MT-Bench,\\\\nand is comparable to 13B \\\\u2013 Chat models. An\\\\nindependent human </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">evaluation was conducted on\\\\nhttps://llmboxing.com/leaderboard.\\\\nIn this evaluation, participants were provided </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with a set of questions along with anonymous responses\\\\nfrom two models and were asked to select their preferred </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">response, as illustrated in Figure 6. As of\\\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">5020 times, compared to 4143\\\\ntimes for Llama 2 13B.\\\\n4\\\\nFigure 5: Results on MMLU, commonsense reasoning, world</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge and reading comprehension for\\\\nMistral 7B and Llama 2 (7B/13B/70B)\\n[Quote from Mistral 7B] \"Mistral </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">7B\\\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\\\nDevendra Singh Chaplot, Diego de las </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Casas, Florian Bressand, Gianna Lengyel,\\\\nGuillaume Lample, Lucile Saulnier, L\\\\u00e9lio Renard Lavaud, Marie-Anne</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lachaux,\\\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\\\\u00e9e Lacroix,\\\\nWilliam El </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Sayed\\\\nAbstract\\\\nWe introduce Mistral 7B, a 7\\\\u2013billion-parameter language model engineered for\\\\nsuperior </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms the best open 13B\\\\nmodel (Llama 2) across all evaluated </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarks, and the best released 34B\\\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model\\\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\\\nwindow attention (SWA)</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to effectively handle sequences of arbitrary length with a\\\\nreduced inference cost\\n'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
              "\u001b[32mjust asked: who is the main author of Mistral 7B paper?\\n\\n From this, we have retrieved the following \u001b[0m\n",
              "\u001b[32mpotentially-useful info:  Conversation History Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with \u001b[0m\n",
              "\u001b[32mTell me a joke.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent previously responded with I\\'m here to help answer questions about \u001b[0m\n",
              "\u001b[32mdocuments, but I can certainly share a joke with you! Here it is:\\n\\nWhy don\\'t scientists trust atoms?\\n\\nBecause \u001b[0m\n",
              "\u001b[32mthey make up everything!\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mI didn\\'t find any relevant information in the provided document for this joke, it\\'s \u001b[0m\n",
              "\u001b[32mjust a classic one I thought I\\'d share.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Mistral 7B\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Mistral 7B \u001b[0m\n",
              "\u001b[32moutperforms the previous best 13B model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLlama 2, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m across all tested\\\\nbenchmarks, and surpasses the best 34B \u001b[0m\n",
              "\u001b[32mmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLaMa 34B, \u001b[0m\u001b[32m[\u001b[0m\u001b[32m25\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in mathematics and code\\\\ngeneration. Furthermore, Mistral 7B approaches the coding \u001b[0m\n",
              "\u001b[32mperformance of Code-Llama 7B \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\\nwithout sacrificing performance on non-code related benchmarks.\\\\nMistral 7B \u001b[0m\n",
              "\u001b[32mleverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6, 3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\\\nGQA significantly \u001b[0m\n",
              "\u001b[32maccelerates the inference speed, and also reduces the memory requirement during\\\\ndecoding, allowing for higher \u001b[0m\n",
              "\u001b[32mbatch sizes hence higher throughput, a crucial factor for real-time\\\\napplications. In addition, SWA is designed to\u001b[0m\n",
              "\u001b[32mhandle longer sequences more effectively at a reduced\\\\ncomputational cost, thereby alleviating a common limitation\u001b[0m\n",
              "\u001b[32min LLMs. These attention mechanisms\\\\ncollectively contribute to the enhanced performance and efficiency of Mistral\u001b[0m\n",
              "\u001b[32m7B.\\\\narXiv:2310.06825v1  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  10 Oct 2023\\\\nMistral 7B is released under the Apache 2\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32m{\u001b[0m\u001b[32m\\'Published\\': \\'2023-10-10\\', \\'Title\\': \\'Mistral 7B\\', \\'Authors\\': \\'Albert Q. Jiang, Alexandre Sablayrolles, \u001b[0m\n",
              "\u001b[32mArthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, \u001b[0m\n",
              "\u001b[32mGuillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut \u001b[0m\n",
              "\u001b[32mLavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed\\', \\'Summary\\': \\'We introduce Mistral 7B v0.1, a \u001b[0m\n",
              "\u001b[32m7-billion-parameter language model engineered\\\\nfor superior performance and efficiency. Mistral 7B outperforms \u001b[0m\n",
              "\u001b[32mLlama 2 13B\\\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\\\ncode generation. \u001b[0m\n",
              "\u001b[32mOur model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for faster\\\\ninference, coupled with sliding window attention \u001b[0m\n",
              "\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\\\nsequences of arbitrary length with a reduced inference cost. We also provide \u001b[0m\n",
              "\u001b[32ma\\\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\\\nthe Llama 2 13B -- Chat model\u001b[0m\n",
              "\u001b[32mboth on human and automated benchmarks. Our\\\\nmodels are released under the Apache 2.0 license.\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
              "\u001b[32mMistral 7B\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .\\\\nNo proprietary data or training tricks were utilized:\\\\nMistral 7B \\\\u2013 Instruct model is a \u001b[0m\n",
              "\u001b[32msimple and\\\\npreliminary demonstration that the base model can\\\\neasily be fine-tuned to achieve good \u001b[0m\n",
              "\u001b[32mperformance.\\\\nIn Table 3, we observe that the resulting model,\\\\nMistral 7B \\\\u2013 Instruct, exhibits superior \u001b[0m\n",
              "\u001b[32mperfor-\\\\nmance compared to all 7B models on MT-Bench,\\\\nand is comparable to 13B \\\\u2013 Chat models. \u001b[0m\n",
              "\u001b[32mAn\\\\nindependent human evaluation was conducted on\\\\nhttps://llmboxing.com/leaderboard.\\\\nIn this evaluation, \u001b[0m\n",
              "\u001b[32mparticipants were provided with a set of questions along with anonymous responses\\\\nfrom two models and were asked \u001b[0m\n",
              "\u001b[32mto select their preferred response, as illustrated in Figure 6. As of\\\\nOctober 6, 2023, the outputs generated by \u001b[0m\n",
              "\u001b[32mMistral 7B were preferred 5020 times, compared to 4143\\\\ntimes for Llama 2 13B.\\\\n4\\\\nFigure 5: Results on MMLU, \u001b[0m\n",
              "\u001b[32mcommonsense reasoning, world knowledge and reading comprehension for\\\\nMistral 7B and Llama 2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7B/13B/70B\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote \u001b[0m\n",
              "\u001b[32mfrom Mistral 7B\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \"Mistral 7B\\\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\\\nDevendra \u001b[0m\n",
              "\u001b[32mSingh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\\\nGuillaume Lample, Lucile Saulnier, \u001b[0m\n",
              "\u001b[32mL\\\\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\\\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, \u001b[0m\n",
              "\u001b[32mTimoth\\\\u00e9e Lacroix,\\\\nWilliam El Sayed\\\\nAbstract\\\\nWe introduce Mistral 7B, a 7\\\\u2013billion-parameter \u001b[0m\n",
              "\u001b[32mlanguage model engineered for\\\\nsuperior performance and efficiency. Mistral 7B outperforms the best open \u001b[0m\n",
              "\u001b[32m13B\\\\nmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLlama 2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m across all evaluated benchmarks, and the best released 34B\\\\nmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLlama 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in reasoning, \u001b[0m\n",
              "\u001b[32mmathematics, and code generation. Our model\\\\nleverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for faster inference, coupled\u001b[0m\n",
              "\u001b[32mwith sliding\\\\nwindow attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle sequences of arbitrary length with a\\\\nreduced \u001b[0m\n",
              "\u001b[32minference cost\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources that are used. Make your response \u001b[0m\n",
              "\u001b[32mconversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'who is the main author of Mistral 7B paper?'\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: who is the main author of Mistral 7B paper?\\n\\n From this, we have retrieved the following </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">potentially-useful info:  Conversation History Retrieval:\\n[Quote from Document] User previously responded with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Tell me a joke.\\n[Quote from Document] Agent previously responded with I\\'m here to help answer questions about </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">documents, but I can certainly share a joke with you! Here it is:\\n\\nWhy don\\'t scientists trust atoms?\\n\\nBecause </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">they make up everything!\\n\\n(I didn\\'t find any relevant information in the provided document for this joke, it\\'s </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">just a classic one I thought I\\'d share.)\\n\\n\\n Document Retrieval:\\n[Quote from Mistral 7B] . Mistral 7B </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">outperforms the previous best 13B model (Llama 2, [26]) across all tested\\\\nbenchmarks, and surpasses the best 34B </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model (LLaMa 34B, [25]) in mathematics and code\\\\ngeneration. Furthermore, Mistral 7B approaches the coding </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">performance of Code-Llama 7B [20],\\\\nwithout sacrificing performance on non-code related benchmarks.\\\\nMistral 7B </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3].\\\\nGQA significantly </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">accelerates the inference speed, and also reduces the memory requirement during\\\\ndecoding, allowing for higher </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">batch sizes hence higher throughput, a crucial factor for real-time\\\\napplications. In addition, SWA is designed to</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">handle longer sequences more effectively at a reduced\\\\ncomputational cost, thereby alleviating a common limitation</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">in LLMs. These attention mechanisms\\\\ncollectively contribute to the enhanced performance and efficiency of Mistral</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">7B.\\\\narXiv:2310.06825v1  [cs.CL]  10 Oct 2023\\\\nMistral 7B is released under the Apache 2\\n[Quote from Document] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">{\\'Published\\': \\'2023-10-10\\', \\'Title\\': \\'Mistral 7B\\', \\'Authors\\': \\'Albert Q. Jiang, Alexandre Sablayrolles, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed\\', \\'Summary\\': \\'We introduce Mistral 7B v0.1, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">7-billion-parameter language model engineered\\\\nfor superior performance and efficiency. Mistral 7B outperforms </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 2 13B\\\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\\\ncode generation. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Our model leverages grouped-query attention (GQA) for faster\\\\ninference, coupled with sliding window attention </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">(SWA) to effectively handle\\\\nsequences of arbitrary length with a reduced inference cost. We also provide </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a\\\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\\\nthe Llama 2 13B -- Chat model</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">both on human and automated benchmarks. Our\\\\nmodels are released under the Apache 2.0 license.\\'}\\n[Quote from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Mistral 7B] .\\\\nNo proprietary data or training tricks were utilized:\\\\nMistral 7B \\\\u2013 Instruct model is a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">simple and\\\\npreliminary demonstration that the base model can\\\\neasily be fine-tuned to achieve good </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">performance.\\\\nIn Table 3, we observe that the resulting model,\\\\nMistral 7B \\\\u2013 Instruct, exhibits superior </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">perfor-\\\\nmance compared to all 7B models on MT-Bench,\\\\nand is comparable to 13B \\\\u2013 Chat models. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">An\\\\nindependent human evaluation was conducted on\\\\nhttps://llmboxing.com/leaderboard.\\\\nIn this evaluation, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">participants were provided with a set of questions along with anonymous responses\\\\nfrom two models and were asked </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to select their preferred response, as illustrated in Figure 6. As of\\\\nOctober 6, 2023, the outputs generated by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Mistral 7B were preferred 5020 times, compared to 4143\\\\ntimes for Llama 2 13B.\\\\n4\\\\nFigure 5: Results on MMLU, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">commonsense reasoning, world knowledge and reading comprehension for\\\\nMistral 7B and Llama 2 (7B/13B/70B)\\n[Quote </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">from Mistral 7B] \"Mistral 7B\\\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\\\nDevendra </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\\\nGuillaume Lample, Lucile Saulnier, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">L\\\\u00e9lio Renard Lavaud, Marie-Anne Lachaux,\\\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Timoth\\\\u00e9e Lacroix,\\\\nWilliam El Sayed\\\\nAbstract\\\\nWe introduce Mistral 7B, a 7\\\\u2013billion-parameter </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language model engineered for\\\\nsuperior performance and efficiency. Mistral 7B outperforms the best open </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">13B\\\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\\\nmodel (Llama 1) in reasoning, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">mathematics, and code generation. Our model\\\\nleverages grouped-query attention (GQA) for faster inference, coupled</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with sliding\\\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\\\nreduced </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">inference cost\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conversational.)'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'who is the main author of Mistral 7B paper?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3d245c691af92680ca.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 4:** Saving Your Index For Evaluation\n",
        "\n",
        "After you've implemented your RAG chain, please save your accumulated vector store as shown [in the official documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading). You'll have a chance to use it again for your final assessment!"
      ],
      "metadata": {
        "id": "tWmE2a1ckPok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save and compress your index\n",
        "docstore.save_local(\"docstore_index\")\n",
        "!tar czvf docstore_index.tgz docstore_index\n",
        "\n",
        "!rm -rf docstore_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hbR89ZjjydV",
        "outputId": "28359cd5-577a-49be-f258-dce5e347ee47"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.pkl\n",
            "docstore_index/index.faiss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything was properly saved, the following line can be invoked to pull the index from the compressed `tgz` file (assuming the pip requirements are installed). After you have confirmed that the cell can pull in your index, download `docstore_index.tgz` for use in the last notebook!"
      ],
      "metadata": {
        "id": "FNfvydWAkZKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "!tar xzvf docstore_index.tgz\n",
        "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
        "docs = new_db.similarity_search(\"Testing the index\")\n",
        "print(docs[0].page_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eoFDX-6kUu8",
        "outputId": "17f601be-2e26-4898-a8f0-4c35517c3669"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.pkl\n",
            "docstore_index/index.faiss\n",
            ". To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7\\nTable 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality\\nSpeci\\ufb01city\\nBART better\\n7.1%\\n16.8%\\nRAG better\\n42.7%\\n37.4%\\nBoth good\\n11.7%\\n11.8%\\nBoth poor\\n17.7%\\n6.9%\\nNo majority\\n20.8%\\n20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO\\nJeopardy QGen\\nGold\\n89.6%\\n90.0%\\nBART\\n70.7%\\n32.4%\\nRAG-Token\\n77.8%\\n46.8%\\nRAG-Seq.\\n83.5%\\n53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classi\\ufb01cation task, both RAG models are equivalent.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nJeopardy-QGen\\nMSMarco\\nFVR-3\\nFVR-2\\nExact Match\\nB-1\\nQB-1\\nR-L\\nB-1\\nLabel Accuracy\\nRAG-Token-BM25\\n29.7\\n41.5\\n32.1\\n33.1\\n17.5\\n22.3\\n55.5\\n48.4\\n75.1\\n91.6\\nRAG-Sequence-BM25\\n31.8\\n44.1\\n36.6\\n33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6185keGkgef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}