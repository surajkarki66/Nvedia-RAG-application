{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Evaluation\n",
        "\n",
        " In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
        "\n",
        "### **Environment Setup:**"
      ],
      "metadata": {
        "id": "xlyB77PqZ46q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu ragas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SDbVhnPaTQT",
        "outputId": "e56707a1-c492-4273-d5f3-b9207686304a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m163.8/163.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/362.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "z0FnPPcYc75P"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "norm_style = Style(bold=True)\n",
        "pprint = partial(console.print, style=base_style)\n",
        "pprint2 = partial(console.print, style=norm_style)"
      ],
      "metadata": {
        "id": "sH1jfhHkaa5t"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
      ],
      "metadata": {
        "id": "pr3NkDoHabv3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "## **1:** Pre-Release Evaluation\n",
        "\n",
        "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
        "\n",
        "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
        "\n",
        "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
        "\n",
        "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
        "\n",
        "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
        "\n",
        "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
        "\n",
        "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
        "\n",
        "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
        "\n",
        "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
        "\n",
        "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
        "\n",
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **2:** LLM-as-a-Judge Formulation\n",
        "\n",
        "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
        "\n",
        "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
        "\n",
        "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
        "\n",
        "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
        "\n",
        "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
        "\n",
        "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
        "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
        "- [**LangChain Evaluators**](https://python.langchain.com/docs/guides/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
        "\n",
        "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
      ],
      "metadata": {
        "id": "CqgFdTjtai2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **3: [Assessment Prep]** Pairwise Evaluator\n",
        "\n",
        "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/docs/guides/evaluation/examples/comparisons).\n",
        "\n",
        "**To prepare for our RAG chain evaluation, we will need to:**\n",
        "\n",
        "- Pull in our document index (the one we saved in the previous notebook).\n",
        "- Recreate our RAG pipeline of choice.\n",
        "\n",
        "**We will specifically be implementing a judge formulation with the following steps:**\n",
        "\n",
        "- Sample the RAG agent document pool to find two document chunks.\n",
        "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
        "- Use the RAG agent to generate its own answer.\n",
        "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
        "\n",
        "**The chain should be a simple but powerful process that tests for the following objective:**\n",
        "\n",
        "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n"
      ],
      "metadata": {
        "id": "wDWCsl_rbuq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### **Task 1:** Pull In Your Document Retrieval Index\n",
        "\n",
        "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
      ],
      "metadata": {
        "id": "bSORmKnPbzzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Make sure you have docstore_index.tgz in your working directory\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "!tar xzvf docstore_index.tgz\n",
        "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
        "docs = list(docstore.docstore._dict.values())\n",
        "\n",
        "def format_chunk(doc):\n",
        "    return (\n",
        "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
        "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
        "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
        "    )\n",
        "\n",
        "## This printout just confirms that your store has been retrieved\n",
        "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
        "pprint(f\"Sample Chunk:\")\n",
        "print(format_chunk(docs[len(docs)//2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "V1GJUTI4agXU",
        "outputId": "0e467f5b-f545-412a-9c51-fd7727aa413e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.pkl\n",
            "docstore_index/index.faiss\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m613\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">613</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper: ReAct: Synergizing Reasoning and Acting in Language Models\n",
            "\n",
            "Summary: While large language models (LLMs) have demonstrated impressive capabilities\n",
            "across tasks in language understanding and interactive decision making, their\n",
            "abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\n",
            "action plan generation) have primarily been studied as separate topics. In this\n",
            "paper, we explore the use of LLMs to generate both reasoning traces and\n",
            "task-specific actions in an interleaved manner, allowing for greater synergy\n",
            "between the two: reasoning traces help the model induce, track, and update\n",
            "action plans as well as handle exceptions, while actions allow it to interface\n",
            "with external sources, such as knowledge bases or environments, to gather\n",
            "additional information. We apply our approach, named ReAct, to a diverse set of\n",
            "language and decision making tasks and demonstrate its effectiveness over\n",
            "state-of-the-art baselines, as well as improved human interpretability and\n",
            "trustworthiness over methods without reasoning or acting components.\n",
            "Concretely, on question answering (HotpotQA) and fact verification (Fever),\n",
            "ReAct overcomes issues of hallucination and error propagation prevalent in\n",
            "chain-of-thought reasoning by interacting with a simple Wikipedia API, and\n",
            "generates human-like task-solving trajectories that are more interpretable than\n",
            "baselines without reasoning traces. On two interactive decision making\n",
            "benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\n",
            "reinforcement learning methods by an absolute success rate of 34% and 10%\n",
            "respectively, while being prompted with only one or two in-context examples.\n",
            "Project site with code: https://react-lm.github.io\n",
            "\n",
            "Page Body: 17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align\n",
            "with these human thought edits and succeed in the task. From a human perspective, solving such a\n",
            "task becomes signiﬁcantly easier, from typing tens of actions to only editing a couple of thoughts,\n",
            "which enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\n",
            "14\n",
            "Published as a conference paper at ICLR 2023\n",
            "\u000bD\f\u00035H$FW\u0003\n",
            "\u0011\u0011\u0011\u0011\u0011\u0011\n",
            "2EV\u0003\u0014\u0019\u001d\u0003<RX\u0003RSHQ\u0003WKH\u0003GUDZHU\u0003\u0017\u0011\u00037KH\u0003GUDZHU\u0003\u0017\u0003LV\u0003RSHQ\u0011\u0003,Q\u0003LW\u000f\u0003\\RX\n",
            "\u0003\n",
            "VHH\u0003D\u0003NH\\FKDLQ\u0003\u0016\u000f\u0003DQG\u0003D\u0003ZDWFK\u0003\u0015\u0011 \u0003\n",
            "$FW\u0003\u0014\u001a\u001d\u00037KLQN\u0003>1RZ\u0003,\u0003ILQG\u0003WKH\u0003ILUVW\u0003NH\\FKDLQ\u0003\u000b\u0016\f\u0011\u00031H[W\u000f\u0003,\u0003QHHG\u0003WR\n",
            "\u0003\n",
            "WDNH\u0003LW\u0011\u0003,\u0003FDQ\u0003ILQG\u0003WKH\u0003VHFRQG\u0003NH\\FKDLQ\u0003\u000b\u0015\f\u0003ODWHU\u0003LQ\u0003GUDZHU\u0003\u0017\u0011\n",
            "\u0003@\n",
            "$FW\u0003\u0014\u001b\u001d\u0003WDNH\u0003NH\\FKDLQ\u0003\u0016\u0003IURP\u0003GUDZHU\u0003\u0017 \u0003\n",
            "2EV\u0003\u0014\u001b\u001d\u0003<RX\u0003SLFN\u0003XS\u0003WKH\u0003NH\\FKDLQ\u0003\u0016\u0003IURP\u0003WKH\u0003GUDZHU\u0003\u0017\u0011\n",
            "\u0003\n",
            "\u0011\u0011\u0011\u0011\u0011\u0011\n",
            "$FW\u0003\u0015\u0015\u001d\u0003SXW\u0003NH\\FKDLQ\u0003\u0016\u0003LQ\u0012RQ\u0003VDIH\u0003\u0014 \u0003\n",
            "2EV\u0003\u0015\u0015\u001d\u0003<RX\u0003SXW\u0003WKH\u0003NH\\FKDLQ\u0003\u0016\u0003LQ\u0012RQ\u0003WKH\u0003VDIH\u0003\u0014\u0011 \u0003\n",
            "$FW\u0003\u0015\u0016\u001d\u00037KLQN\u0003>1RZ\u0003,\u0003SXW\u0003WKH\u0003ILUVW\u0003NH\\FKDLQ\u0003LQ\u0003VDIH\u0011\u00031H[W\u000f\u0003,\u0003QHHG\n",
            "\u0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the manual embedding strategy from the previous notebook is still very viable, but we can also rest easy and let a **vector store** do all that work for us!\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "VuQz76vvcH1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Task 2:** Pull In Your RAG Chain\n",
        "\n",
        "Now that we have our index, we can recreate the RAG agent from the previous notebook!\n",
        "\n",
        "**Key Modifications:**\n",
        "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
      ],
      "metadata": {
        "id": "iwrypWIwcNKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain.document_transformers import LongContextReorder\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "llm = instruct_llm | StrOutputParser()\n",
        "\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
        "    \" The following information may be useful for your response: \"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
        "    \"\\n\\nUser Question: {input}\"\n",
        ")\n",
        "\n",
        "def output_puller(inputs):\n",
        "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
        "    if isinstance(inputs, dict):\n",
        "        inputs = [inputs]\n",
        "    for token in inputs:\n",
        "        if token.get('output'):\n",
        "            yield token.get('output')\n",
        "\n",
        "## Pull in your desired RAG Chain. Memory not necessary\n",
        "\n",
        "## Chain Specs: \"Hello World\" -> retrieval_chain\n",
        "##   -> {'input': <str>, 'context' : <str>}\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
        "# context_getter = RunnableLambda(lambda x: x)  ## TODO\n",
        "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str\n",
        "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
        "\n",
        "## Chain Specs: retrieval_chain -> generator_chain\n",
        "##   -> {\"output\" : <str>, ...} -> output_puller\n",
        "# generator_chain = RunnableLambda(lambda x: x)  ## TODO\n",
        "generator_chain = chat_prompt | llm  ## TODO\n",
        "generator_chain = {\"output\" : generator_chain } | RunnableLambda(output_puller)  ## GIVEN\n",
        "\n",
        "rag_chain = retrieval_chain | generator_chain\n",
        "\n",
        "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
        "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
        "    print(token, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvxSS3PacF1k",
        "outputId": "a88a7c2f-c2b1-4dbc-f3fd-4843e4b7141c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I just read about this fascinating concept called the \"ReAct framework\" in a paper titled \"ReAct: Synergizing Reasoning and Acting in Language Models\". It's an approach to document retrieval where a system can comprehend and interact with a document by following a step-by-step process of decision-making and action-taking. Think of it like a robot trying to solve a task, like putting a clean knife on the countertop, by searching for and manipulating objects within a virtual environment. For example, the system might think \"I need to find a clean knife\" and then search for it on different countertops. It's actually quite an interesting way to approach document retrieval and I think you might find it fascinating too!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
        "\n",
        "In this section, we can implement the first few part of our evaluation routine:\n",
        "\n",
        "- **Sample the RAG agent document pool to find two document chunks.**\n",
        "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
        "- Use the RAG agent to generate its own answer.\n",
        "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
        "\n",
        "The chain should be a simple but powerful process that tests for the following objective:\n",
        "\n",
        "> Does my RAG chain outperform a narrow chatbot with limited document access."
      ],
      "metadata": {
        "id": "zaR_Hisp-VoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "num_questions = 3\n",
        "synth_questions = []\n",
        "synth_answers = []\n",
        "\n",
        "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
        "\n",
        "for i in range(num_questions):\n",
        "    doc1, doc2 = random.sample(docs, 2)\n",
        "    sys_msg = (\n",
        "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
        "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
        "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
        "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
        "    )\n",
        "    usr_msg = (\n",
        "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
        "        f\"Document2: {format_chunk(doc2)}\"\n",
        "    )\n",
        "\n",
        "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
        "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
        "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
        "    pprint2(f\"QA Pair {i+1}\")\n",
        "    pprint2(synth_questions[-1])\n",
        "    pprint(synth_answers[-1])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "pIn__8rlcvx6",
        "outputId": "f21ed61a-d163-4b67-8de1-c2a0fea934fc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m not only exhibit reasoning capabilities but also perform tasks by \u001b[0m\n",
              "\u001b[1minteracting with external environments, such as Wikipedia or APIs, to gather additional information?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can large language models (LLMs) not only exhibit reasoning capabilities but also perform tasks by </span>\n",
              "<span style=\"font-weight: bold\">interacting with external environments, such as Wikipedia or APIs, to gather additional information?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: Yes, our approach, named ReAct, demonstrates the ability of LLMs to generate both reasoning traces and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtask-specific actions in an interleaved manner, allowing for greater synergy between the two. We apply ReAct to a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdiverse set of language and decision-making tasks, including question answering and fact verification, and show \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthat it overcomes issues of hallucination and error propagation by interacting with a simple Wikipedia API.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Yes, our approach, named ReAct, demonstrates the ability of LLMs to generate both reasoning traces and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-specific actions in an interleaved manner, allowing for greater synergy between the two. We apply ReAct to a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">diverse set of language and decision-making tasks, including question answering and fact verification, and show </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that it overcomes issues of hallucination and error propagation by interacting with a simple Wikipedia API.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQuestion: Can a pre-trained model that learns from natural language supervision be used to improve performance on \u001b[0m\n",
              "\u001b[1mvarious NLP tasks by leveraging a non-parametric memory and retrieval mechanism?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can a pre-trained model that learns from natural language supervision be used to improve performance on </span>\n",
              "<span style=\"font-weight: bold\">various NLP tasks by leveraging a non-parametric memory and retrieval mechanism?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: Yes, our work, as well as related studies on general-purpose architectures for NLP tasks, have shown that \u001b[0m\n",
              "\u001b[1;38;2;118;185;0msuch models can achieve strong performance on a wide range of tasks, including language generation, question \u001b[0m\n",
              "\u001b[1;38;2;118;185;0manswering, and classification, by incorporating retrieval and non-parametric memory into a single architecture. For\u001b[0m\n",
              "\u001b[1;38;2;118;185;0minstance, in Document1, we demonstrate that learning from natural language supervision enables zero-shot transfer \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mof the model to downstream tasks, and in Document2, we explore a general-purpose fine-tuning recipe for \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mretrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models that use a pre-trained seq2seq model and a dense vector index of \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mWikipedia to access knowledge.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Yes, our work, as well as related studies on general-purpose architectures for NLP tasks, have shown that </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">such models can achieve strong performance on a wide range of tasks, including language generation, question </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">answering, and classification, by incorporating retrieval and non-parametric memory into a single architecture. For</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">instance, in Document1, we demonstrate that learning from natural language supervision enables zero-shot transfer </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of the model to downstream tasks, and in Document2, we explore a general-purpose fine-tuning recipe for </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieval-augmented generation (RAG) models that use a pre-trained seq2seq model and a dense vector index of </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Wikipedia to access knowledge.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQuestion: How do Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models overcome the limitations of pre-trained language \u001b[0m\n",
              "\u001b[1mmodels in terms of accessing and precisely manipulating knowledge, especially on knowledge-intensive tasks?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do Retrieval-Augmented Generation (RAG) models overcome the limitations of pre-trained language </span>\n",
              "<span style=\"font-weight: bold\">models in terms of accessing and precisely manipulating knowledge, especially on knowledge-intensive tasks?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: RAG models combine pre-trained parametric and non-parametric memory for language generation, using a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdifferentiable access mechanism to explicit non-parametric memory, such as a dense vector index of Wikipedia, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0maccessed with a pre-trained neural retriever. This allows the models to draw from a vast amount of knowledge and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mgenerate more specific, diverse, and factual language than parametric-only seq2seq models, as shown in the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mexperiments on language generation tasks.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: RAG models combine pre-trained parametric and non-parametric memory for language generation, using a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">differentiable access mechanism to explicit non-parametric memory, such as a dense vector index of Wikipedia, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">accessed with a pre-trained neural retriever. This allows the models to draw from a vast amount of knowledge and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generate more specific, diverse, and factual language than parametric-only seq2seq models, as shown in the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">experiments on language generation tasks.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### **Step 4:** Answer The Synthetic Questions\n",
        "\n",
        "In this section, we can implement the third part of our evaluation routine:\n",
        "\n",
        "- Sample the RAG agent document pool to find two document chunks.\n",
        "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
        "- **Use the RAG agent to generate its own answer.**\n",
        "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
        "\n",
        "The chain should be a simple but powerful process that tests for the following objective:\n",
        "\n",
        "> Does my RAG chain outperform a narrow chatbot with limited document access."
      ],
      "metadata": {
        "id": "wshz2VGC-2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate some synthetic answers to the questions above.\n",
        "##   Try to use the same syntax as the cell above\n",
        "rag_answers = []\n",
        "for i, q in enumerate(synth_questions):\n",
        "    ## Compute the RAG Answer\n",
        "    rag_answer = \"\"\n",
        "    rag_answer = rag_chain.invoke(q)\n",
        "    rag_answers += [rag_answer]\n",
        "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
        "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hU1yULZBdPTC",
        "outputId": "579d3272-027f-4392-db40-e8cddd2a0db6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
              "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m not only exhibit reasoning capabilities but also perform tasks by \u001b[0m\n",
              "\u001b[1minteracting with external environments, such as Wikipedia or APIs, to gather additional information?\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "<span style=\"font-weight: bold\">Question: Can large language models (LLMs) not only exhibit reasoning capabilities but also perform tasks by </span>\n",
              "<span style=\"font-weight: bold\">interacting with external environments, such as Wikipedia or APIs, to gather additional information?</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: Yes, large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m can definitely exhibit both reasoning capabilities and perform tasks \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mby interacting with external environments such as Wikipedia or APIs to gather additional information.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mIn fact, recent research has shown that LLMs can be designed to combine reasoning and acting with language models \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfor solving diverse tasks. This is often referred to as a paradigm called \u001b[0m\u001b[32m\"ReAct\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSynergizing Reasoning and Acting\u001b[0m\n",
              "\u001b[1;38;2;118;185;0min Language Models\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAs you can see from the document retrieval you provided, ReAct enables LLMs to generate both verbal reasoning \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtraces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mreasoning to create, maintain, and adjust high-level plans for acting, while also interacting with external \u001b[0m\n",
              "\u001b[1;38;2;118;185;0menvironments to incorporate additional information into reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mFor example, as mentioned in one of the quotes, WebGPT uses an LLM to interact with web browsers, navigate through \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mweb pages, and infer answers to complicated questions from ELI5. This example shows how LLMs can use external \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minformation from the web to support reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSimilarly, the document mentions that the authors designed a simple Wikipedia web API with three types of actions \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mto support interactive information retrieval. This was done to simulate how humans would interact with Wikipedia \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand force models to retrieve via explicit reasoning in language. This again shows how LLMs can be designed to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minteract with external environments to gather additional information and support reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSo, to answer your question, yes, LLMs can definitely exhibit reasoning capabilities and perform tasks by \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minteracting with external environments such as Wikipedia or APIs to gather additional information.\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Yes, large language models (LLMs) can definitely exhibit both reasoning capabilities and perform tasks </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">by interacting with external environments such as Wikipedia or APIs to gather additional information.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In fact, recent research has shown that LLMs can be designed to combine reasoning and acting with language models </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for solving diverse tasks. This is often referred to as a paradigm called </span><span style=\"color: #008000; text-decoration-color: #008000\">\"ReAct\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Synergizing Reasoning and Acting</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in Language Models).</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As you can see from the document retrieval you provided, ReAct enables LLMs to generate both verbal reasoning </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">traces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reasoning to create, maintain, and adjust high-level plans for acting, while also interacting with external </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">environments to incorporate additional information into reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">For example, as mentioned in one of the quotes, WebGPT uses an LLM to interact with web browsers, navigate through </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">web pages, and infer answers to complicated questions from ELI5. This example shows how LLMs can use external </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">information from the web to support reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Similarly, the document mentions that the authors designed a simple Wikipedia web API with three types of actions </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to support interactive information retrieval. This was done to simulate how humans would interact with Wikipedia </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and force models to retrieve via explicit reasoning in language. This again shows how LLMs can be designed to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">interact with external environments to gather additional information and support reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to answer your question, yes, LLMs can definitely exhibit reasoning capabilities and perform tasks by </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">interacting with external environments such as Wikipedia or APIs to gather additional information.</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
              "\u001b[1mQuestion: Can a pre-trained model that learns from natural language supervision be used to improve performance on \u001b[0m\n",
              "\u001b[1mvarious NLP tasks by leveraging a non-parametric memory and retrieval mechanism?\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "<span style=\"font-weight: bold\">Question: Can a pre-trained model that learns from natural language supervision be used to improve performance on </span>\n",
              "<span style=\"font-weight: bold\">various NLP tasks by leveraging a non-parametric memory and retrieval mechanism?</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: That's a great question. So, you're looking at pre-trained models that learn from natural language \u001b[0m\n",
              "\u001b[1;38;2;118;185;0msupervision, and how they can be leveraged with a non-parametric memory and retrieval mechanism to improve \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mperformance on various NLP tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mFrom what I've read, pre-trained models with a non-parametric memory component have been investigated for \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mextractive downstream tasks \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRefer to \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. These models can learn to access knowledge from a large \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdatabase, and in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, they found that learned retrieval improves results for open Natural Questions, WebQuestions, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand CuratedTrec.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mBut what about NLP tasks beyond extractive tasks? Well, in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, the authors explored a general-purpose fine-tuning \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrecipe for retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, combining pre-trained parametric and non-parametric memory \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfor language generation. They introduced RAG models with a pre-trained seq2seq model as the parametric memory and a\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdense vector index of Wikipedia as the non-parametric memory, accessed with a pre-trained neural retriever.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAnd the results were impressive! RAG models set the state of the art on three open domain QA tasks, outperforming \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mparametric seq2seq models and task-specific retrieve-and-extract architectures \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRefer to \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m68\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m67\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m66\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. The \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mnon-parametric memory allowed the model to easily update at test time, without needing further training.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSo, to answer your question, yes, a pre-trained model that learns from natural language supervision can be used to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mimprove performance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mWould you like to know more about how this works, or perhaps explore other approaches to leveraging non-parametric \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmemory in NLP tasks?\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: That's a great question. So, you're looking at pre-trained models that learn from natural language </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">supervision, and how they can be leveraged with a non-parametric memory and retrieval mechanism to improve </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance on various NLP tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">From what I've read, pre-trained models with a non-parametric memory component have been investigated for </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">extractive downstream tasks (Refer to [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]). These models can learn to access knowledge from a large </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">database, and in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], they found that learned retrieval improves results for open Natural Questions, WebQuestions, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and CuratedTrec.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">But what about NLP tasks beyond extractive tasks? Well, in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], the authors explored a general-purpose fine-tuning </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">recipe for retrieval-augmented generation (RAG) models, combining pre-trained parametric and non-parametric memory </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for language generation. They introduced RAG models with a pre-trained seq2seq model as the parametric memory and a</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dense vector index of Wikipedia as the non-parametric memory, accessed with a pre-trained neural retriever.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">And the results were impressive! RAG models set the state of the art on three open domain QA tasks, outperforming </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parametric seq2seq models and task-specific retrieve-and-extract architectures (Refer to [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]). The </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">non-parametric memory allowed the model to easily update at test time, without needing further training.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to answer your question, yes, a pre-trained model that learns from natural language supervision can be used to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">improve performance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Would you like to know more about how this works, or perhaps explore other approaches to leveraging non-parametric </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">memory in NLP tasks?</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
              "\u001b[1mQuestion: How do Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models overcome the limitations of pre-trained language \u001b[0m\n",
              "\u001b[1mmodels in terms of accessing and precisely manipulating knowledge, especially on knowledge-intensive tasks?\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "<span style=\"font-weight: bold\">Question: How do Retrieval-Augmented Generation (RAG) models overcome the limitations of pre-trained language </span>\n",
              "<span style=\"font-weight: bold\">models in terms of accessing and precisely manipulating knowledge, especially on knowledge-intensive tasks?</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: RAG models are a game-changer when it comes to addressing the limitations of pre-trained language \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmodels. Specifically, when it comes to accessing and manipulating knowledge, especially on knowledge-intensive \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSo, here's the thing. Pre-trained language models have been shown to learn a substantial amount of knowledge from \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdata \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m47\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, but they can't easily expand or revise their memory. They're like a closed book - you can't peek inside \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mto see how they're making predictions. And if you want to know more about their knowledge, you're stuck.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThat's where RAG models come in. By combining pre-trained parametric and non-parametric memories, RAG models can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0maccess and manipulate knowledge in a more flexible and powerful way.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mHere's how it works. The non-parametric memory is essentially a giant knowledge base, like a dense vector index of \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mWikipedia, that's accessed with a pre-trained neural retriever. This means that RAG models can retrieve relevant \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge from an external source, rather than relying solely on their internal memory.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mBut that's not all. RAG models can also update their knowledge as the world changes, which is a big plus when it \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcomes to knowledge-intensive tasks. They can do this by replacing the non-parametric memory with new knowledge, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwhich is a degree of control that pre-trained language models simply can't match.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mTo give you a better idea, the authors of the paper on RAG models used a variety of knowledge-intensive tasks to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdemonstrate the capabilities of RAG models. For example, they used open domain QA tasks like Natural Questions, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mWebQuestions, and CuratedTrec, where RAG models outperformed parametric seq2seq models and task-specific \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mretrieve-and-extract architectures.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThey also used language generation tasks like MS-MARCO and Jeopardy question generation, where RAG models generated\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmore specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq model.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOverall, RAG models are a powerful approach to overcoming the limitations of pre-trained language models, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mespecially when it comes to accessing and precisely manipulating knowledge on knowledge-intensive tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m47\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Peters, M. E., et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Deep contextualized word representations. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1802.05365\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: RAG models are a game-changer when it comes to addressing the limitations of pre-trained language </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models. Specifically, when it comes to accessing and manipulating knowledge, especially on knowledge-intensive </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, here's the thing. Pre-trained language models have been shown to learn a substantial amount of knowledge from </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">data [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], but they can't easily expand or revise their memory. They're like a closed book - you can't peek inside </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to see how they're making predictions. And if you want to know more about their knowledge, you're stuck.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">That's where RAG models come in. By combining pre-trained parametric and non-parametric memories, RAG models can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">access and manipulate knowledge in a more flexible and powerful way.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Here's how it works. The non-parametric memory is essentially a giant knowledge base, like a dense vector index of </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Wikipedia, that's accessed with a pre-trained neural retriever. This means that RAG models can retrieve relevant </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge from an external source, rather than relying solely on their internal memory.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">But that's not all. RAG models can also update their knowledge as the world changes, which is a big plus when it </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">comes to knowledge-intensive tasks. They can do this by replacing the non-parametric memory with new knowledge, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">which is a degree of control that pre-trained language models simply can't match.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To give you a better idea, the authors of the paper on RAG models used a variety of knowledge-intensive tasks to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">demonstrate the capabilities of RAG models. For example, they used open domain QA tasks like Natural Questions, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">WebQuestions, and CuratedTrec, where RAG models outperformed parametric seq2seq models and task-specific </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieve-and-extract architectures.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">They also used language generation tasks like MS-MARCO and Jeopardy question generation, where RAG models generated</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq model.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, RAG models are a powerful approach to overcoming the limitations of pre-trained language models, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">especially when it comes to accessing and precisely manipulating knowledge on knowledge-intensive tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Peters, M. E., et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Deep contextualized word representations. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1802.05365</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "### **Step 5:** Implement A Human Preference Metric\n",
        "\n",
        "In this section, we can implement the fourth part of our evaluation routine:\n",
        "\n",
        "- Sample the RAG agent document pool to find two document chunks.\n",
        "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
        "- Use the RAG agent to generate its own answer.\n",
        "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
        "\n",
        "The chain should be a simple but powerful process that tests for the following objective:\n",
        "\n",
        "> Does my RAG chain outperform a narrow chatbot with limited document access."
      ],
      "metadata": {
        "id": "WF0yk16w_Ecc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Adapt this prompt for whichever LLM you're actually interested in using.\n",
        "## If it's llama, maybe system message would be good?\n",
        "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION:\n",
        "Evaluate the following Question-Answer pair for human preference and consistency.\n",
        "Assume the first answer is a ground truth answer and has to be correct.\n",
        "Assume the second answer may or may not be true.\n",
        "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
        "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
        "\n",
        "Output Format:\n",
        "[Score] Justification\n",
        "\n",
        "{qa_trio}\n",
        "\n",
        "EVALUATION:\n",
        "\"\"\")\n",
        "\n",
        "pref_score = []\n",
        "\n",
        "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
        "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
        "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
        "\n",
        "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
        "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
        "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
        "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
        "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "urAMMzIQdaT2",
        "outputId": "49670a1d-bc13-4e33-f696-80bbc2abe05e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m not only exhibit reasoning capabilities but also perform tasks\u001b[0m\n",
              "\u001b[1mby interacting with external environments, such as Wikipedia or APIs, to gather additional information?\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: Can large language models (LLMs) not only exhibit reasoning capabilities but also perform tasks</span>\n",
              "<span style=\"font-weight: bold\">by interacting with external environments, such as Wikipedia or APIs, to gather additional information?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Yes, our approach, named ReAct, demonstrates the ability of LLMs to generate both reasoning \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtraces and task-specific actions in an interleaved manner, allowing for greater synergy between the two. We apply \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mReAct to a diverse set of language and decision-making tasks, including question answering and fact verification, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand show that it overcomes issues of hallucination and error propagation by interacting with a simple Wikipedia \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mAPI.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Yes, our approach, named ReAct, demonstrates the ability of LLMs to generate both reasoning </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two. We apply </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ReAct to a diverse set of language and decision-making tasks, including question answering and fact verification, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and show that it overcomes issues of hallucination and error propagation by interacting with a simple Wikipedia </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">API.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: Yes, large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m can definitely exhibit both reasoning capabilities and perform tasks \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mby interacting with external environments such as Wikipedia or APIs to gather additional information.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mIn fact, recent research has shown that LLMs can be designed to combine reasoning and acting with language models \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfor solving diverse tasks. This is often referred to as a paradigm called \u001b[0m\u001b[32m\"ReAct\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSynergizing Reasoning and Acting\u001b[0m\n",
              "\u001b[1;38;2;118;185;0min Language Models\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAs you can see from the document retrieval you provided, ReAct enables LLMs to generate both verbal reasoning \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtraces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mreasoning to create, maintain, and adjust high-level plans for acting, while also interacting with external \u001b[0m\n",
              "\u001b[1;38;2;118;185;0menvironments to incorporate additional information into reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mFor example, as mentioned in one of the quotes, WebGPT uses an LLM to interact with web browsers, navigate through \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mweb pages, and infer answers to complicated questions from ELI5. This example shows how LLMs can use external \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minformation from the web to support reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSimilarly, the document mentions that the authors designed a simple Wikipedia web API with three types of actions \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mto support interactive information retrieval. This was done to simulate how humans would interact with Wikipedia \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand force models to retrieve via explicit reasoning in language. This again shows how LLMs can be designed to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minteract with external environments to gather additional information and support reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSo, to answer your question, yes, LLMs can definitely exhibit reasoning capabilities and perform tasks by \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minteracting with external environments such as Wikipedia or APIs to gather additional information.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Yes, large language models (LLMs) can definitely exhibit both reasoning capabilities and perform tasks </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">by interacting with external environments such as Wikipedia or APIs to gather additional information.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In fact, recent research has shown that LLMs can be designed to combine reasoning and acting with language models </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for solving diverse tasks. This is often referred to as a paradigm called </span><span style=\"color: #008000; text-decoration-color: #008000\">\"ReAct\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Synergizing Reasoning and Acting</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in Language Models).</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As you can see from the document retrieval you provided, ReAct enables LLMs to generate both verbal reasoning </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">traces and actions pertaining to a task in an interleaved manner. This allows the model to perform dynamic </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reasoning to create, maintain, and adjust high-level plans for acting, while also interacting with external </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">environments to incorporate additional information into reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">For example, as mentioned in one of the quotes, WebGPT uses an LLM to interact with web browsers, navigate through </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">web pages, and infer answers to complicated questions from ELI5. This example shows how LLMs can use external </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">information from the web to support reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Similarly, the document mentions that the authors designed a simple Wikipedia web API with three types of actions </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to support interactive information retrieval. This was done to simulate how humans would interact with Wikipedia </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and force models to retrieve via explicit reasoning in language. This again shows how LLMs can be designed to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">interact with external environments to gather additional information and support reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to answer your question, yes, LLMs can definitely exhibit reasoning capabilities and perform tasks by </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">interacting with external environments such as Wikipedia or APIs to gather additional information.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m Justification\u001b[0m\n",
              "\n",
              "\u001b[1mThe second answer is essentially a rephrased and detailed version of the first answer, providing more evidence and \u001b[0m\n",
              "\u001b[1mexamples to support the idea that large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m can exhibit reasoning capabilities and perform tasks\u001b[0m\n",
              "\u001b[1mby interacting with external environments, such as Wikipedia or APIs. The additional information provided in the \u001b[0m\n",
              "\u001b[1msecond answer does not introduce any inconsistencies with the first answer and actually reinforces the ground \u001b[0m\n",
              "\u001b[1mtruth. The second answer is therefore considered to be better than the first and does not lie or fail to answer the\u001b[0m\n",
              "\u001b[1mquestion.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> Justification</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">The second answer is essentially a rephrased and detailed version of the first answer, providing more evidence and </span>\n",
              "<span style=\"font-weight: bold\">examples to support the idea that large language models (LLMs) can exhibit reasoning capabilities and perform tasks</span>\n",
              "<span style=\"font-weight: bold\">by interacting with external environments, such as Wikipedia or APIs. The additional information provided in the </span>\n",
              "<span style=\"font-weight: bold\">second answer does not introduce any inconsistencies with the first answer and actually reinforces the ground </span>\n",
              "<span style=\"font-weight: bold\">truth. The second answer is therefore considered to be better than the first and does not lie or fail to answer the</span>\n",
              "<span style=\"font-weight: bold\">question.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: Can a pre-trained model that learns from natural language supervision be used to improve \u001b[0m\n",
              "\u001b[1mperformance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism?\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: Can a pre-trained model that learns from natural language supervision be used to improve </span>\n",
              "<span style=\"font-weight: bold\">performance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Yes, our work, as well as related studies on general-purpose architectures for NLP tasks, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mhave shown that such models can achieve strong performance on a wide range of tasks, including language generation,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mquestion answering, and classification, by incorporating retrieval and non-parametric memory into a single \u001b[0m\n",
              "\u001b[1;38;2;118;185;0marchitecture. For instance, in Document1, we demonstrate that learning from natural language supervision enables \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mzero-shot transfer of the model to downstream tasks, and in Document2, we explore a general-purpose fine-tuning \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrecipe for retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models that use a pre-trained seq2seq model and a dense vector \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mindex of Wikipedia to access knowledge.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Yes, our work, as well as related studies on general-purpose architectures for NLP tasks, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">have shown that such models can achieve strong performance on a wide range of tasks, including language generation,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question answering, and classification, by incorporating retrieval and non-parametric memory into a single </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architecture. For instance, in Document1, we demonstrate that learning from natural language supervision enables </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">zero-shot transfer of the model to downstream tasks, and in Document2, we explore a general-purpose fine-tuning </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">recipe for retrieval-augmented generation (RAG) models that use a pre-trained seq2seq model and a dense vector </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">index of Wikipedia to access knowledge.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: That's a great question. So, you're looking at pre-trained models that learn from natural language \u001b[0m\n",
              "\u001b[1;38;2;118;185;0msupervision, and how they can be leveraged with a non-parametric memory and retrieval mechanism to improve \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mperformance on various NLP tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mFrom what I've read, pre-trained models with a non-parametric memory component have been investigated for \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mextractive downstream tasks \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRefer to \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. These models can learn to access knowledge from a large \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdatabase, and in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, they found that learned retrieval improves results for open Natural Questions, WebQuestions, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand CuratedTrec.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mBut what about NLP tasks beyond extractive tasks? Well, in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, the authors explored a general-purpose fine-tuning \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrecipe for retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, combining pre-trained parametric and non-parametric memory \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfor language generation. They introduced RAG models with a pre-trained seq2seq model as the parametric memory and a\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdense vector index of Wikipedia as the non-parametric memory, accessed with a pre-trained neural retriever.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAnd the results were impressive! RAG models set the state of the art on three open domain QA tasks, outperforming \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mparametric seq2seq models and task-specific retrieve-and-extract architectures \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRefer to \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m68\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m67\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m66\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. The \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mnon-parametric memory allowed the model to easily update at test time, without needing further training.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSo, to answer your question, yes, a pre-trained model that learns from natural language supervision can be used to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mimprove performance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mWould you like to know more about how this works, or perhaps explore other approaches to leveraging non-parametric \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmemory in NLP tasks?\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: That's a great question. So, you're looking at pre-trained models that learn from natural language </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">supervision, and how they can be leveraged with a non-parametric memory and retrieval mechanism to improve </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance on various NLP tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">From what I've read, pre-trained models with a non-parametric memory component have been investigated for </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">extractive downstream tasks (Refer to [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]). These models can learn to access knowledge from a large </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">database, and in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], they found that learned retrieval improves results for open Natural Questions, WebQuestions, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and CuratedTrec.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">But what about NLP tasks beyond extractive tasks? Well, in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], the authors explored a general-purpose fine-tuning </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">recipe for retrieval-augmented generation (RAG) models, combining pre-trained parametric and non-parametric memory </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for language generation. They introduced RAG models with a pre-trained seq2seq model as the parametric memory and a</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dense vector index of Wikipedia as the non-parametric memory, accessed with a pre-trained neural retriever.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">And the results were impressive! RAG models set the state of the art on three open domain QA tasks, outperforming </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parametric seq2seq models and task-specific retrieve-and-extract architectures (Refer to [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]). The </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">non-parametric memory allowed the model to easily update at test time, without needing further training.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, to answer your question, yes, a pre-trained model that learns from natural language supervision can be used to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">improve performance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Would you like to know more about how this works, or perhaps explore other approaches to leveraging non-parametric </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">memory in NLP tasks?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m Justification\u001b[0m\n",
              "\n",
              "\u001b[1mThe second answer is generally better than the first and does not introduce any inconsistencies. Here's why:\u001b[0m\n",
              "\n",
              "\u001b[1;36m1\u001b[0m\u001b[1m. The second answer provides a more detailed and specific explanation of how pre-trained models with \u001b[0m\n",
              "\u001b[1mnon-parametric memory can be used to improve performance on various NLP tasks.\u001b[0m\n",
              "\u001b[1;36m2\u001b[0m\u001b[1m. The answer cites specific studies and papers that demonstrate the effectiveness of this approach, such as \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\n",
              "\u001b[1mand \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m68\u001b[0m\u001b[1m, \u001b[0m\u001b[1;36m67\u001b[0m\u001b[1m, \u001b[0m\u001b[1;36m66\u001b[0m\u001b[1m]\u001b[0m\u001b[1m.\u001b[0m\n",
              "\u001b[1;36m3\u001b[0m\u001b[1m. The second answer also provides more context and elaboration on the application of this approach to tasks beyond\u001b[0m\n",
              "\u001b[1mextractive tasks, including language generation and open-domain QA.\u001b[0m\n",
              "\u001b[1;36m4\u001b[0m\u001b[1m. The answer is well-structured and clear, with a logical flow of ideas and a direct response to the question.\u001b[0m\n",
              "\n",
              "\u001b[1mOverall, the second answer provides a more nuanced and informative response to the question, while still confirming\u001b[0m\n",
              "\u001b[1mthe original answer that a pre-trained model that learns from natural language supervision can be used to improve \u001b[0m\n",
              "\u001b[1mperformance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> Justification</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">The second answer is generally better than the first and does not introduce any inconsistencies. Here's why:</span>\n",
              "\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. The second answer provides a more detailed and specific explanation of how pre-trained models with </span>\n",
              "<span style=\"font-weight: bold\">non-parametric memory can be used to improve performance on various NLP tasks.</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">. The answer cites specific studies and papers that demonstrate the effectiveness of this approach, such as [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">] </span>\n",
              "<span style=\"font-weight: bold\">and [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span><span style=\"font-weight: bold\">].</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">. The second answer also provides more context and elaboration on the application of this approach to tasks beyond</span>\n",
              "<span style=\"font-weight: bold\">extractive tasks, including language generation and open-domain QA.</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">. The answer is well-structured and clear, with a logical flow of ideas and a direct response to the question.</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Overall, the second answer provides a more nuanced and informative response to the question, while still confirming</span>\n",
              "<span style=\"font-weight: bold\">the original answer that a pre-trained model that learns from natural language supervision can be used to improve </span>\n",
              "<span style=\"font-weight: bold\">performance on various NLP tasks by leveraging a non-parametric memory and retrieval mechanism.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: How do Retrieval-Augmented Generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models overcome the limitations of pre-trained \u001b[0m\n",
              "\u001b[1mlanguage models in terms of accessing and precisely manipulating knowledge, especially on knowledge-intensive \u001b[0m\n",
              "\u001b[1mtasks?\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: How do Retrieval-Augmented Generation (RAG) models overcome the limitations of pre-trained </span>\n",
              "<span style=\"font-weight: bold\">language models in terms of accessing and precisely manipulating knowledge, especially on knowledge-intensive </span>\n",
              "<span style=\"font-weight: bold\">tasks?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: RAG models combine pre-trained parametric and non-parametric memory for language generation, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0musing a differentiable access mechanism to explicit non-parametric memory, such as a dense vector index of \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mWikipedia, accessed with a pre-trained neural retriever. This allows the models to draw from a vast amount of \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge and generate more specific, diverse, and factual language than parametric-only seq2seq models, as shown \u001b[0m\n",
              "\u001b[1;38;2;118;185;0min the experiments on language generation tasks.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: RAG models combine pre-trained parametric and non-parametric memory for language generation, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">using a differentiable access mechanism to explicit non-parametric memory, such as a dense vector index of </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Wikipedia, accessed with a pre-trained neural retriever. This allows the models to draw from a vast amount of </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge and generate more specific, diverse, and factual language than parametric-only seq2seq models, as shown </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in the experiments on language generation tasks.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: RAG models are a game-changer when it comes to addressing the limitations of pre-trained language \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmodels. Specifically, when it comes to accessing and manipulating knowledge, especially on knowledge-intensive \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mSo, here's the thing. Pre-trained language models have been shown to learn a substantial amount of knowledge from \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdata \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m47\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, but they can't easily expand or revise their memory. They're like a closed book - you can't peek inside \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mto see how they're making predictions. And if you want to know more about their knowledge, you're stuck.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThat's where RAG models come in. By combining pre-trained parametric and non-parametric memories, RAG models can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0maccess and manipulate knowledge in a more flexible and powerful way.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mHere's how it works. The non-parametric memory is essentially a giant knowledge base, like a dense vector index of \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mWikipedia, that's accessed with a pre-trained neural retriever. This means that RAG models can retrieve relevant \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge from an external source, rather than relying solely on their internal memory.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mBut that's not all. RAG models can also update their knowledge as the world changes, which is a big plus when it \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcomes to knowledge-intensive tasks. They can do this by replacing the non-parametric memory with new knowledge, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwhich is a degree of control that pre-trained language models simply can't match.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mTo give you a better idea, the authors of the paper on RAG models used a variety of knowledge-intensive tasks to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdemonstrate the capabilities of RAG models. For example, they used open domain QA tasks like Natural Questions, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mWebQuestions, and CuratedTrec, where RAG models outperformed parametric seq2seq models and task-specific \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mretrieve-and-extract architectures.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThey also used language generation tasks like MS-MARCO and Jeopardy question generation, where RAG models generated\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmore specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq model.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOverall, RAG models are a powerful approach to overcoming the limitations of pre-trained language models, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mespecially when it comes to accessing and precisely manipulating knowledge on knowledge-intensive tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m47\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Peters, M. E., et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Deep contextualized word representations. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1802.05365\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: RAG models are a game-changer when it comes to addressing the limitations of pre-trained language </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models. Specifically, when it comes to accessing and manipulating knowledge, especially on knowledge-intensive </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">So, here's the thing. Pre-trained language models have been shown to learn a substantial amount of knowledge from </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">data [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], but they can't easily expand or revise their memory. They're like a closed book - you can't peek inside </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to see how they're making predictions. And if you want to know more about their knowledge, you're stuck.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">That's where RAG models come in. By combining pre-trained parametric and non-parametric memories, RAG models can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">access and manipulate knowledge in a more flexible and powerful way.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Here's how it works. The non-parametric memory is essentially a giant knowledge base, like a dense vector index of </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Wikipedia, that's accessed with a pre-trained neural retriever. This means that RAG models can retrieve relevant </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge from an external source, rather than relying solely on their internal memory.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">But that's not all. RAG models can also update their knowledge as the world changes, which is a big plus when it </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">comes to knowledge-intensive tasks. They can do this by replacing the non-parametric memory with new knowledge, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">which is a degree of control that pre-trained language models simply can't match.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To give you a better idea, the authors of the paper on RAG models used a variety of knowledge-intensive tasks to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">demonstrate the capabilities of RAG models. For example, they used open domain QA tasks like Natural Questions, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">WebQuestions, and CuratedTrec, where RAG models outperformed parametric seq2seq models and task-specific </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieve-and-extract architectures.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">They also used language generation tasks like MS-MARCO and Jeopardy question generation, where RAG models generated</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq model.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, RAG models are a powerful approach to overcoming the limitations of pre-trained language models, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">especially when it comes to accessing and precisely manipulating knowledge on knowledge-intensive tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Peters, M. E., et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2018</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Deep contextualized word representations. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1802.05365</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1m Justification\u001b[0m\n",
              "\n",
              "\u001b[1mAnswer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m \u001b[0m\u001b[1m(\u001b[0m\u001b[1mGround Truth\u001b[0m\u001b[1m)\u001b[0m\u001b[1m provides a specific and technical explanation of how RAG models overcome the limitations of\u001b[0m\n",
              "\u001b[1mpre-trained language models, including the use of a differentiable access mechanism to explicit non-parametric \u001b[0m\n",
              "\u001b[1mmemory and the combination of pre-trained parametric and non-parametric memory. This answer is consistent with the \u001b[0m\n",
              "\u001b[1moriginal prompt and provides a clear and detailed explanation of the topic.\u001b[0m\n",
              "\n",
              "\u001b[1mAnswer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m \u001b[0m\u001b[1m(\u001b[0m\u001b[1mNew Answer\u001b[0m\u001b[1m)\u001b[0m\u001b[1m attempts to provide a similar explanation but is less specific and technical, and introduces \u001b[0m\n",
              "\u001b[1mnew points that are not mentioned in the original prompt. While it mentions the limitations of pre-trained language\u001b[0m\n",
              "\u001b[1mmodels and the capabilities of RAG models, it does not provide a clear and detailed explanation of the specific \u001b[0m\n",
              "\u001b[1mmechanisms used by RAG models. Additionally, Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m quotes an external reference \u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m47\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m that is not mentioned in \u001b[0m\n",
              "\u001b[1mthe original prompt, which is not relevant to the topic and introduces an inconsistency.\u001b[0m\n",
              "\n",
              "\u001b[1mTherefore, Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m \u001b[0m\u001b[1m(\u001b[0m\u001b[1mGround Truth\u001b[0m\u001b[1m)\u001b[0m\u001b[1m is a more accurate and consistent answer to the question, and Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m \u001b[0m\u001b[1m(\u001b[0m\u001b[1mNew \u001b[0m\n",
              "\u001b[1mAnswer\u001b[0m\u001b[1m)\u001b[0m\u001b[1m is inferior and introduces inconsistencies. The score is \u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1m, indicating that Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m is the correct and \u001b[0m\n",
              "\u001b[1mpreferred answer.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\"> Justification</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> (Ground Truth) provides a specific and technical explanation of how RAG models overcome the limitations of</span>\n",
              "<span style=\"font-weight: bold\">pre-trained language models, including the use of a differentiable access mechanism to explicit non-parametric </span>\n",
              "<span style=\"font-weight: bold\">memory and the combination of pre-trained parametric and non-parametric memory. This answer is consistent with the </span>\n",
              "<span style=\"font-weight: bold\">original prompt and provides a clear and detailed explanation of the topic.</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> (New Answer) attempts to provide a similar explanation but is less specific and technical, and introduces </span>\n",
              "<span style=\"font-weight: bold\">new points that are not mentioned in the original prompt. While it mentions the limitations of pre-trained language</span>\n",
              "<span style=\"font-weight: bold\">models and the capabilities of RAG models, it does not provide a clear and detailed explanation of the specific </span>\n",
              "<span style=\"font-weight: bold\">mechanisms used by RAG models. Additionally, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> quotes an external reference ([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"font-weight: bold\">]) that is not mentioned in </span>\n",
              "<span style=\"font-weight: bold\">the original prompt, which is not relevant to the topic and introduces an inconsistency.</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Therefore, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> (Ground Truth) is a more accurate and consistent answer to the question, and Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> (New </span>\n",
              "<span style=\"font-weight: bold\">Answer) is inferior and introduces inconsistencies. The score is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">, indicating that Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> is the correct and </span>\n",
              "<span style=\"font-weight: bold\">preferred answer.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
      ],
      "metadata": {
        "id": "R4kwXSsQAbNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
        "print(f\"Preference Score: {pref_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W819EjroAeiA",
        "outputId": "7905d23a-99e2-4eb0-9e1b-6549ba051fec"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preference Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **4:** Advanced Formulations\n",
        "\n",
        "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action.\n",
        "\n",
        "With that being said, this metric was merely a product of us specifying:\n",
        "- **What kind of behavior is important for our pipeline to have?**\n",
        "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
        "\n",
        "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
        "\n",
        "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
        "\n",
        "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
        "\n",
        "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/latest/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine.\n",
        "\n",
        "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
        "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/docs/guides/evaluation/trajectory/) is a good starting point.\n",
        "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/docs/modules/agents/concepts) is a good starting point!\n",
        "\n",
        "<br>\n",
        "\n",
        "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
      ],
      "metadata": {
        "id": "WQWQeZJrA8yt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KImtbKs9AgEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}