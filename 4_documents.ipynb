{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Large Documents\n",
        "In this notebook, we will take the same ideas and move towards the space of large documents, considering what kinds of issues we will run into as we try to incorporate large files into our LLM contexts.\n"
      ],
      "metadata": {
        "id": "_9EKpTrNfnex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup"
      ],
      "metadata": {
        "id": "A0ErkKaPjoPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Necessary for Colab, not necessary for course environment\n",
        "%pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
        "%pip install -qq arxiv pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEOTrVg8jnCL",
        "outputId": "659d5fca-0a33-46ee-c9c9-b0c594f3d995"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m637.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m477.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "Zuz2EO44jzJE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ],
      "metadata": {
        "id": "cHVmjbY1j1r8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "ChatNVIDIA.get_available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "362M91U-j56F",
        "outputId": "93962f92-e4f0-41c1-e176-afcc0005822e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/paligemma', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='liuhaotian/llava-v1.6-34b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/stg/vlm/community/llava16-34b', aliases=['ai-llava16-34b', 'community/llava16-34b', 'liuhaotian/llava16-34b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/kosmos-2', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='liuhaotian/llava-v1.6-mistral-7b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/stg/vlm/community/llava16-mistral-7b', aliases=['ai-llava16-mistral-7b', 'community/llava16-mistral-7b', 'liuhaotian/llava16-mistral-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/neva-22b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/deplot', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='adept/fuyu-8b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Useful utility method for printing intermediate states\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from functools import partial\n",
        "\n",
        "def RPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def PPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        pprint(preface, x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ],
      "metadata": {
        "id": "Ej6w2tXfkscP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1:** Chatting with Documents\n",
        "\n",
        "This notebook will begin a longer stream of discussion surrounding the use of LLMs to chat with documents. In a world where chat models are trained on giant repositories of public data and retraining them on custom data is prohibitively expensive, the idea of having an LLM reason about a set of PDFs or even a YouTube video opens up many opportunities!\n",
        "\n",
        "- **Your LLM can have a modifiable knowledge base grounded in human-readible documents,** meaning that you can directly control what kinds of data it has access to and can instruct it to interact with it.\n",
        "\n",
        "- **Your LLM can sort through and pull references directly from your document set.** With sufficient prompt engineering and instruction-following priors, you can force your models to only act based on the material you provide.\n",
        "\n",
        "- **Your LLM can possibly even interact with your documents, making automatic modifications as necessary.** This opens up avenues in automatic content refinement and synthetic operations which will be explored later.\n",
        "\n",
        "Listing out some possibilities is pretty easy, and from there you can let your imagination run wild... but we haven't gained the tools to do this quite yet, right?\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Naive Approach: Stuff Your Document**\n",
        "\n",
        "Suppose you have some text documents (PDF, blog, etc.) and want to ask questions related to the contents of those documents. One approach you could try involves taking a representation of the document and feeding it all to a chat model! From a document perspective, this is known as [**document stuffing**](https://python.langchain.com/docs/modules/chains/document/stuff).\n",
        "\n",
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=14DRI_uDviqzqg14TKoIc8IlBc3Zsb8oO\" width=800px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_stuff.png\" width=800px/>\n",
        ">\n",
        "> From [**Stuff | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/chains/document/stuff)\n",
        "\n",
        "<br>\n",
        "\n",
        "This may very well work if your model is strong enough and if your document is short enough, but it shouldn't be expected to work well for an entire document. Many modern LLMs have significant trouble working with long contexts due to training limitations. Nowadays large model deterioration isn't quite as catastrophic, but good instruction following is likely to fall apart pretty quickly regardless of which model you use (assuming you're accessing the raw model).\n",
        "\n",
        "<br>\n",
        "\n",
        "**The key issues you'll need to resolve with document reasoning are:**\n",
        "\n",
        "- How do we split our documents into pieces that can be reasoned with?\n",
        "\n",
        "- How can we find and consider these pieces efficiently as the size and number of documents increases?\n",
        "\n",
        "\n",
        "Speaking of, the field of document loading frameworks has many strong options, and two major players will come up throughout the course:\n",
        "\n",
        "- [**LangChain**](https://python.langchain.com/docs/get_started/introduction) provides a simple framework for connecting LLMs to your own data sources via general chunking strategies and strong incorporation with embedding frameworks/services. This framework has initially grown around its strong general support for LLM features, which signals its active strengths closer to the chain abstractions and agent coordination.\n",
        "\n",
        "- [**LlamaIndex**](https://gpt-index.readthedocs.io/en/stable/) is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. It has since branched out to include general LLM capabilities similar to LangChain, but as of now it is still strongest in addressing the document side of LLM components since its initial abstractions were centered around that problem.\n",
        "\n",
        "It's recommended to read more about the unique strengths of both LlamaIndex and LangChain and pick the one that works best for you. Since LlamaIndex can be used *with* LangChain, the frameworks' unique capabilities [can be leveraged together without too much issue](https://docs.llamaindex.ai/en/stable/community/integrations/using_with_langchain.html). For the sake of simplicity, we will stick to LangChain in this course and will allow the [**NVIDIA/GenerativeAIExamples repository**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration/notebooks) to explore deeper LlamaIndex options for those interested.\n"
      ],
      "metadata": {
        "id": "oOwleCx7k217"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2:** Loading Documents\n",
        "\n",
        "LangChain provides a variety of [document loaders](https://python.langchain.com/docs/integrations/document_loaders) to facilitate the injestion of various document formats (HTML, PDF, code) from many different sources and locations (local storage, private s3 buckets, public websites, messaging APIs, etc.). These loaders query your data sources and return a `Document` object which contains the content and metadata, usually in a plain-text or otherwise human-readible format. There are plenty of document loaders already built and ready to use, with the first-party LangChain options listed [here](https://python.langchain.com/docs/integrations/document_loaders).\n",
        "\n",
        "**In this example, we can load a research paper of our choice using one of the following LangChain loaders:**\n",
        "- [`UnstructuredFileLoader`](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file): Generally-useful file loader for arbitrary files; doesn't make too many assumptions about your document structure and is usually sufficient.\n",
        "- [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv): A more specialized file-loader which can communicate with the Arxiv interface directly. [Just one example of many](https://python.langchain.com/docs/integrations/document_loaders), this will make some more assumptions about your data to yield nicer parsings and auto-fill metadata (useful when you have multiple documents/formats).\n",
        "\n",
        "For our code example we will default to using `ArxivLoader` to load in one of either the [MRKL](https://arxiv.org/abs/2205.00445) or [ReAct](https://arxiv.org/abs/2210.03629) publication papers as you're likely to run into them at some point in your continued chat model research endeavors."
      ],
      "metadata": {
        "id": "vBQ4czLEmKyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUaDvEr-mkEZ",
        "outputId": "35389e34-184d-428a-dad1-b45385a89f5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.34)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.104)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.12 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "## Loading in the file\n",
        "\n",
        "## Unstructured File Loader: Good for arbitrary \"probably good enough\" loader\n",
        "# documents = UnstructuredFileLoader(\"llama2_paper.pdf\").load()\n",
        "\n",
        "## More specialized loader, won't work for everything, but simple API and usually better results\n",
        "documents = ArxivLoader(query=\"2404.16130\").load()  ## GraphRAG\n",
        "# documents = ArxivLoader(query=\"2404.03622\").load()  ## Visualization-of-Thought\n",
        "# documents = ArxivLoader(query=\"2404.19756\").load()  ## KAN: Kolmogorov-Arnold Networks\n",
        "# documents = ArxivLoader(query=\"2404.07143\").load()  ## Infini-Attention\n",
        "# documents = ArxivLoader(query=\"2210.03629\").load()  ## ReAct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WcgtbQ5ky-c",
        "outputId": "161691f5-ae61-4b8f-f48c-0d869810c763"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 85.9 ms, sys: 20.2 ms, total: 106 ms\n",
            "Wall time: 412 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Printing out a sample of the content\n",
        "print(\"Number of Documents Retrieved:\", len(documents))\n",
        "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
        "print(documents[0].page_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFhDG8tMmhsd",
        "outputId": "38383bd1-3911-48f7-a91a-153c62f8b225"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents Retrieved: 1\n",
            "Sample of Document 1 Content (Total Length: 53880):\n",
            "From Local to Global: A Graph RAG Approach to\n",
            "Query-Focused Summarization\n",
            "Darren Edge1†\n",
            "Ha Trinh1†\n",
            "Newman Cheng2\n",
            "Joshua Bradley2\n",
            "Alex Chao3\n",
            "Apurva Mody3\n",
            "Steven Truitt2\n",
            "Jonathan Larson1\n",
            "1Microsoft Research\n",
            "2Microsoft Strategic Missions and Technologies\n",
            "3Microsoft Office of the CTO\n",
            "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n",
            "@microsoft.com\n",
            "†These authors contributed equally to this work\n",
            "Abstract\n",
            "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
            "tion from an external knowledge source enables large language models (LLMs)\n",
            "to answer questions over private and/or previously unseen document collections.\n",
            "However, RAG fails on global questions directed at an entire text corpus, such\n",
            "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
            "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
            "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\n",
            "RAG systems. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(documents[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "YxzSUEqLmwrr",
        "outputId": "7fa03931-9ba4-4601-8cfc-81739f03774f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-04-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, \u001b[0m\n",
              "\u001b[32mJonathan Larson'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to retrieve relevant\\ninformation from an external \u001b[0m\n",
              "\u001b[32mknowledge source enables large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to answer questions over private and/or previously unseen \u001b[0m\n",
              "\u001b[32mdocument\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are\u001b[0m\n",
              "\u001b[32mthe main themes in the dataset?\", since this is\\ninherently a query-focused summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task, rather than \u001b[0m\n",
              "\u001b[32man explicit\\nretrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities\\nof text indexed by \u001b[0m\n",
              "\u001b[32mtypical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose a Graph RAG approach to \u001b[0m\n",
              "\u001b[32mquestion answering over\\nprivate text corpora that scales with both the generality of user questions and\\nthe \u001b[0m\n",
              "\u001b[32mquantity of source text to be indexed. Our approach uses an LLM to build a\\ngraph-based text index in two stages: \u001b[0m\n",
              "\u001b[32mfirst to derive an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for \u001b[0m\n",
              "\u001b[32mall\\ngroups of closely-related entities. Given a question, each community summary is\\nused to generate a partial \u001b[0m\n",
              "\u001b[32mresponse, before all partial responses are again\\nsummarized in a final response to the user. For a class of global\u001b[0m\n",
              "\u001b[32msensemaking\\nquestions over datasets in the 1 million token range, we show that Graph RAG\\nleads to substantial \u001b[0m\n",
              "\u001b[32mimprovements over a na\\\\\"ive RAG baseline for both the\\ncomprehensiveness and diversity of generated answers. An \u001b[0m\n",
              "\u001b[32mopen-source,\\nPython-based implementation of both global and local Graph RAG approaches is\\nforthcoming at \u001b[0m\n",
              "\u001b[32mhttps://aka.ms/graphrag.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-04-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Jonathan Larson'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of retrieval-augmented generation (RAG) to retrieve relevant\\ninformation from an external </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source enables large language models\\n(LLMs) to answer questions over private and/or previously unseen </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">document\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the main themes in the dataset?\", since this is\\ninherently a query-focused summarization (QFS) task, rather than </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">an explicit\\nretrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities\\nof text indexed by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose a Graph RAG approach to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">question answering over\\nprivate text corpora that scales with both the generality of user questions and\\nthe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">quantity of source text to be indexed. Our approach uses an LLM to build a\\ngraph-based text index in two stages: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">first to derive an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">all\\ngroups of closely-related entities. Given a question, each community summary is\\nused to generate a partial </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">response, before all partial responses are again\\nsummarized in a final response to the user. For a class of global</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sensemaking\\nquestions over datasets in the 1 million token range, we show that Graph RAG\\nleads to substantial </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">improvements over a na\\\\\"ive RAG baseline for both the\\ncomprehensiveness and diversity of generated answers. An </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">open-source,\\nPython-based implementation of both global and local Graph RAG approaches is\\nforthcoming at </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">https://aka.ms/graphrag.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Though it may be tempting to accept the metadata format as-is and ignore the body entirely, there are a key selection of features that cannot be approached without diving into the full text:\n",
        "\n",
        "- **The metadata is not guaranteed.** In the case of `arxiv`, paper abstracts, titles, authors, and date are necessary components of a submission, so being able to query them is not surprising. For an arbitrary PDF or webpage though, the same is not necessarily the case.\n",
        "- **The agent will not be able to go deeper into the document content.** The summary is good to know and can be used as-is, but does not provide a straight-forward path to interacting with the body at any capacity (at least not from what we've learned).\n",
        "- **The agent will still not be able to reason about too many documents at once.** Perhaps in the MRKL/ReAct example, you could combine those two summaries into one context and ask some questions. But what happens when you need to interact with 5 documents at once? What about an entire directory? Very soon, you will notice that your context window will be overloaded with information just to summarize or even list out the documents you're interested in!"
      ],
      "metadata": {
        "id": "c-5e4NkOm646"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## **3:** Transforming The Documents\n",
        "\n",
        "Once documents have been loaded, they often need to be transformed if we intend to pass them into our LLMs as context. One method of transformation is known as **chunking**, which breaks down large pieces of content into smaller segments. This technique is valuable because it helps [optimize the relevance of the content returned from the vector database](https://www.pinecone.io/learn/chunking-strategies/).\n",
        "\n",
        "LangChain provides a [variety of document transformers](https://python.langchain.com/docs/integrations/document_transformers/) out of which we will use the [``RecursiveCharacterTextSplitter``](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter). This option will allow us to split our document with preference for some natural stopping points that we want our chunks to follow (as much as possible)."
      ],
      "metadata": {
        "id": "2EkowF2JnG_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "## Some nice custom preprocessing\n",
        "documents[0].page_content = documents[0].page_content.replace(\". .\", \"\")\n",
        "docs_split = text_splitter.split_documents(documents)\n",
        "\n",
        "def include_doc(doc):\n",
        "    ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
        "    string = doc.page_content\n",
        "    if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
        "print(len(docs_split))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuC6_woem2Fh",
        "outputId": "98249845-c254-41ec-e2fd-9c2f0bd9bfba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in (0, 1, 2, 15, -1):\n",
        "    pprint(f\"[Document {i}]\")\n",
        "    print(docs_split[i].page_content)\n",
        "    pprint(\"=\"*64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "93q1RneYnnSI",
        "outputId": "0c6d1f97-180c-43f6-dcde-c87c8bb5194c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From Local to Global: A Graph RAG Approach to\n",
            "Query-Focused Summarization\n",
            "Darren Edge1†\n",
            "Ha Trinh1†\n",
            "Newman Cheng2\n",
            "Joshua Bradley2\n",
            "Alex Chao3\n",
            "Apurva Mody3\n",
            "Steven Truitt2\n",
            "Jonathan Larson1\n",
            "1Microsoft Research\n",
            "2Microsoft Strategic Missions and Technologies\n",
            "3Microsoft Office of the CTO\n",
            "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso}\n",
            "@microsoft.com\n",
            "†These authors contributed equally to this work\n",
            "Abstract\n",
            "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
            "tion from an external knowledge source enables large language models (LLMs)\n",
            "to answer questions over private and/or previously unseen document collections.\n",
            "However, RAG fails on global questions directed at an entire text corpus, such\n",
            "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
            "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
            "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\n",
            "RAG systems. To combine the strengths of these contrasting methods, we propose\n",
            "a Graph RAG approach to question answering over private text corpora that scales\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a Graph RAG approach to question answering over private text corpora that scales\n",
            "with both the generality of user questions and the quantity of source text to be in-\n",
            "dexed. Our approach uses an LLM to build a graph-based text index in two stages:\n",
            "first to derive an entity knowledge graph from the source documents, then to pre-\n",
            "generate community summaries for all groups of closely-related entities. Given a\n",
            "question, each community summary is used to generate a partial response, before\n",
            "all partial responses are again summarized in a final response to the user. For a\n",
            "class of global sensemaking questions over datasets in the 1 million token range,\n",
            "we show that Graph RAG leads to substantial improvements over a na¨\n",
            "ıve RAG\n",
            "baseline for both the comprehensiveness and diversity of generated answers. An\n",
            "open-source, Python-based implementation of both global and local Graph RAG\n",
            "approaches is forthcoming at https://aka.ms/graphrag.\n",
            "1\n",
            "Introduction\n",
            "Human endeavors across a range of domains rely on our ability to read and reason about large\n",
            "collections of documents, often reaching conclusions that go beyond anything stated in the source\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collections of documents, often reaching conclusions that go beyond anything stated in the source\n",
            "texts themselves. With the emergence of large language models (LLMs), we are already witnessing\n",
            "attempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\n",
            "crosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\n",
            "Preprint. Under review.\n",
            "arXiv:2404.16130v1  [cs.CL]  24 Apr 2024\n",
            "Source Documents\n",
            "Text Chunks\n",
            "text extraction\n",
            "and chunking\n",
            "Element Instances\n",
            "domain-tailored\n",
            "summarization\n",
            "Element Summaries\n",
            "domain-tailored\n",
            "summarization\n",
            "Graph Communities\n",
            "community\n",
            "detection\n",
            "Community Summaries\n",
            "domain-tailored\n",
            "summarization\n",
            "Community Answers\n",
            "query-focused\n",
            "summarization\n",
            "Global Answer\n",
            "query-focused\n",
            "summarization\n",
            "Indexing Time\n",
            "Query Time\n",
            "Pipeline Stage\n",
            "Figure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\n",
            "index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have\n",
            "been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6\n",
            "Community Summaries →Community Answers →Global Answer\n",
            "Given a user query, the community summaries generated in the previous step can be used to generate\n",
            "a final answer in a multi-stage process. The hierarchical nature of the community structure also\n",
            "means that questions can be answered using the community summaries from different levels, raising\n",
            "the question of whether a particular level in the hierarchical community structure offers the best\n",
            "balance of summary detail and scope for general sensemaking questions (evaluated in section 3).\n",
            "For a given community level, the global answer to any user query is generated as follows:\n",
            "• Prepare community summaries. Community summaries are randomly shuffled and divided\n",
            "into chunks of pre-specified token size. This ensures relevant information is distributed\n",
            "across chunks, rather than concentrated (and potentially lost) in a single context window.\n",
            "• Map community answers. Generate intermediate answers in parallel, one for each chunk.\n",
            "The LLM is also asked to generate a score between 0-100 indicating how helpful the gen-\n",
            "erated answer is in answering the target question. Answers with score 0 are filtered out.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on\n",
            "Empirical Methods in Natural Language Processing (EMNLP).\n",
            "Yao, J.-g., Wan, X., and Xiao, J. (2017). Recent advances in document summarization. Knowledge\n",
            "and Information Systems, 53:297–336.\n",
            "14\n",
            "Yao, L., Peng, J., Mao, C., and Luo, Y. (2023). Exploring large language models for knowledge\n",
            "graph completion.\n",
            "Zhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt\n",
            "augmented by chatgpt. arXiv preprint arXiv:2304.11116.\n",
            "Zhang, Y., Zhang, Y., Gan, Y., Yao, L., and Wang, C. (2024). Causal graph discovery with retrieval-\n",
            "augmented generation based large language models. arXiv preprint arXiv:2402.15301.\n",
            "Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing,\n",
            "E., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\n",
            "Information Processing Systems, 36.\n",
            "15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "Our approach for chunking is pretty naive, but highlights the ease of getting at least something working for our application. We made some effort to keep the chunk size small so that our models are able to wield it effectively as context, but how are we going to reason about all of these pieces?\n",
        "\n",
        "**When extending and optimizing this approach for an arbitrary set of documents, some potential options include:**\n",
        "\n",
        "- Identifying logical breaks or synthesis techniques (manually, automatically, LLM-assisted, etc).\n",
        "- Aiming to construct chunks that are rich in unique and relevant information, avoiding redundancy to maximize database utility.\n",
        "- Customizing chunking to fit the document’s nature, ensuring the chunks are contextually relevant and cohesive.\n",
        "- Including key concepts, keywords, or metadata snippets in each chunk for improved searchability and relevance in the database.\n",
        "- Continuously assessing chunking effectiveness and be ready to adjust strategies for optimal balance between size and content richness.\n",
        "- Considering a hierarchy system (implicitly-generated or explicitly-specified) to improve retrieval attempts.\n",
        "    - If interested, please look over the [**LlamaIndex tree structures from the index guide**](https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide.html#tree-index) as a starting point."
      ],
      "metadata": {
        "id": "b3h-VwWrn_xT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## **4:** Refining Summaries\n",
        "\n",
        "To automatically reason about large documents, one potential idea might be to use LLMs to create a dense summary or knowledge base. Similar to how we maintained a running history of the conversation via slot-filling in the previous notebook, is there any problem with keeping a running history of an entire document?\n",
        "\n",
        "In this section, we focus on an exciting application of LLMs: **automatically refining, coercing, and consolidating data en masse**. Specifically, we'll be implementing a simple but useful Runnable that uses a while loop and the running state chain formulation to summarize a set of document chunks. This process is commonly known as [**\"document refinement\"**](https://python.langchain.com/docs/modules/chains/document/refine) and is largely akin to our previous conversation-focused slot-filling exercise; the only difference is that now we're dealing with a large document instead of a growing chat history.\n",
        "\n",
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1J2XR8Cc8YSkVJMiJCknMkgA02mBT8riZ\" width=1000px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/doc_refine.png\" width=1000px/>\n",
        ">\n",
        "> From [**Refine | LangChain**🦜️🔗](https://python.langchain.com/docs/modules/chains/document/refine)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **The DocumentSummaryBase Model**\n",
        "\n",
        "Much like the `KnowledgeBase` class from the previous notebook, we can create a `DocumentSummaryBase` structure designed to encapsulate the essence of a document. The one below will use the `running_summary` field to query the model for a final summary while attempting to use the `main_ideas` and `loose_ends` fields as a bottleneck to keep the running summary from moving too fast. This is something we're going to have to enforce via prompt engineering, so the `summary_prompt` is also provided which shows how this information will be used. Feel free to modify it as necessary to make it work for your model of choice."
      ],
      "metadata": {
        "id": "LY-xGUukoIHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import List\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "class DocumentSummaryBase(BaseModel):\n",
        "    running_summary: str = Field(\"\", description=\"Running description of the document. Do not override; only update!\")\n",
        "    main_ideas: List[str] = Field([], description=\"Most important information from the document (max 3)\")\n",
        "    loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
        "\n",
        "\n",
        "summary_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
        "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
        "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
        "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
        "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
        "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZJgnlVKCnqP2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        # print(string)  ## Good for diagnostics\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser\n"
      ],
      "metadata": {
        "id": "LFj7jggeoyhq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_summary = \"\"\n",
        "\n",
        "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
        "    '''\n",
        "    Create a chain that summarizes\n",
        "    '''\n",
        "    def summarize_docs(docs):\n",
        "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
        "        ## Initialize a valid starting state.\n",
        "        state = {'info_base' : knowledge}\n",
        "\n",
        "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
        "\n",
        "        for i, doc in enumerate(docs):\n",
        "            ## Update the state as appropriate using your parse_chain component\n",
        "            state['input'] = doc.page_content\n",
        "            state = parse_chain.invoke(state)\n",
        "\n",
        "            assert 'info_base' in state\n",
        "            if verbose:\n",
        "                print(f\"Considered {i+1} documents\")\n",
        "                pprint(state['info_base'])\n",
        "                latest_summary = state['info_base']\n",
        "                clear_output(wait=True)\n",
        "\n",
        "        return state['info_base']\n",
        "\n",
        "    return RunnableLambda(summarize_docs)\n",
        "\n",
        "# instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\").bind(max_tokens=4096)\n",
        "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
        "instruct_llm = instruct_model | StrOutputParser()\n",
        "\n",
        "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
        "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
        "summary = summarizer.invoke(docs_split[:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "b8oY_ES4o1r5",
        "outputId": "c0711801-e5db-4611-ede1-f1f8c5d7ca74"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Considered 15 documents\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Graph RAG, a novel approach, merges the strengths of RAG and traditional QFS for question \u001b[0m\n",
              "\u001b[32manswering over private text corpora. It scales with both the generality of user questions and the quantity of \u001b[0m\n",
              "\u001b[32msource text to be indexed. Using an LLM, Graph RAG generates a graph-based text index, modeled as an undirected \u001b[0m\n",
              "\u001b[32mweighted graph. The index is partitioned into communities using the Leiden algorithm to efficiently recover \u001b[0m\n",
              "\u001b[32mhierarchical community structure. Community summaries are independently useful and scalable. Graph RAG outperforms \u001b[0m\n",
              "\u001b[32mthe RAG baseline for global sensemaking questions, balancing recall and precision during element extraction. \u001b[0m\n",
              "\u001b[32mCommunity summaries enable understanding of global dataset structure and can be used as part of the graph-based \u001b[0m\n",
              "\u001b[32mindex for answering global queries. Hierarchical clustering involves two levels: Level 0, corresponding to the \u001b[0m\n",
              "\u001b[32mhierarchical partition with maximum modularity, and Level 1, which reveals internal structure within these \u001b[0m\n",
              "\u001b[32mroot-level communities. In leaf-level communities, element summaries are prioritized and iteratively added to the \u001b[0m\n",
              "\u001b[32mLLM context window until the token limit is reached, prioritizing in decreasing order of combined source and target\u001b[0m\n",
              "\u001b[32mnode degree. For higher-level communities, if all element summaries fit within the token limit, the process is \u001b[0m\n",
              "\u001b[32msimilar to that of leaf-level communities. Otherwise, sub-communities are ranked in decreasing order of element \u001b[0m\n",
              "\u001b[32msummary tokens, and sub-community summaries are iteratively substituted for their associated element summaries \u001b[0m\n",
              "\u001b[32muntil they fit within the context window.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG approach proposed to combine RAG and QFS strengths, enabling question answering over private \u001b[0m\n",
              "\u001b[32mtext corpora.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Graph RAG scales with both the generality of user questions and the quantity of source text to be \u001b[0m\n",
              "\u001b[32mindexed.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The necessity of balancing recall and precision during element extraction for the target activity.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does expanding LLM context windows affect information retention in longer contexts?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How can the map-reduce approach enable query-focused abstractive summarization when dealing with large \u001b[0m\n",
              "\u001b[32mtext volumes?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What impact do hierarchical levels of community summaries have on user understanding of broad issues and \u001b[0m\n",
              "\u001b[32mthemes?'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG, a novel approach, merges the strengths of RAG and traditional QFS for question </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">answering over private text corpora. It scales with both the generality of user questions and the quantity of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">source text to be indexed. Using an LLM, Graph RAG generates a graph-based text index, modeled as an undirected </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">weighted graph. The index is partitioned into communities using the Leiden algorithm to efficiently recover </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">hierarchical community structure. Community summaries are independently useful and scalable. Graph RAG outperforms </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the RAG baseline for global sensemaking questions, balancing recall and precision during element extraction. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Community summaries enable understanding of global dataset structure and can be used as part of the graph-based </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">index for answering global queries. Hierarchical clustering involves two levels: Level 0, corresponding to the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">hierarchical partition with maximum modularity, and Level 1, which reveals internal structure within these </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">root-level communities. In leaf-level communities, element summaries are prioritized and iteratively added to the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">LLM context window until the token limit is reached, prioritizing in decreasing order of combined source and target</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">node degree. For higher-level communities, if all element summaries fit within the token limit, the process is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">similar to that of leaf-level communities. Otherwise, sub-communities are ranked in decreasing order of element </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">summary tokens, and sub-community summaries are iteratively substituted for their associated element summaries </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">until they fit within the context window.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG approach proposed to combine RAG and QFS strengths, enabling question answering over private </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">text corpora.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Graph RAG scales with both the generality of user questions and the quantity of source text to be </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">indexed.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The necessity of balancing recall and precision during element extraction for the target activity.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does expanding LLM context windows affect information retention in longer contexts?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How can the map-reduce approach enable query-focused abstractive summarization when dealing with large </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">text volumes?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What impact do hierarchical levels of community summaries have on user understanding of broad issues and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">themes?'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bA8_NP1ZpeaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}