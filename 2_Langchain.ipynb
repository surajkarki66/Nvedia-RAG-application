{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain\n",
        "LangChain is an popular LLM orchestration library to help set up systems that have one or more LLM components. The library is, for better or worse, extremely popular and changes rapidly based on new developments in the field, meaning that somebody can have a lot of experience in some parts of LangChain while having little-to-no familiarity with other parts (either because there are just so many different features or the area is new and the features have only recently been implemented).\n",
        "\n",
        "This experiment will be using the **LangChain Expression Language (LCEL)** to ramp up from basic chain specification to more advanced dialog management practices, so hopefully the journey will be enjoyable and even seasoned LangChain developers might learn something new!"
      ],
      "metadata": {
        "id": "_5gLNflLtk9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- > <img style=\"max-width: 400px;\" src=\"imgs/langchain-diagram.png\" /> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/langchain-diagram.png\" width=400px/>\n",
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1NS7dmLf5ql04o5CyPZnd1gnXXgO8-jbR\" width=400px/> -->"
      ],
      "metadata": {
        "id": "AXU3vER821f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment Setup:**"
      ],
      "metadata": {
        "id": "jpeXk7a31IdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Necessary for Colab, not necessary for course environment\n",
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
        "\n",
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA_YFnYj1LZS",
        "outputId": "9d44aa2a-ba2b-43e1-9e36-186dbefc0fd1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains and Runnables\n",
        "\n",
        "When exploring a new library, it's important to note what are the core systems of the library and how are they used.\n",
        "\n",
        "In LangChain, the main building block *used to be* the classic **Chain**: a small module of functionality that does something specific and can be linked up with other chains to make a system. So for all intents and purposes, it is a \"building-block system\" abstraction where the building blocks are easy to create, have consistent methods (`invoke`, `generate`, `stream`, etc), and can be linked up to work together as a system. Some example legacy chains include `LLMChain`, `ConversationChain`, `TransformationChain`, `SequentialChain`, etc.\n",
        "\n",
        "More recently, a new recommended specification has emerged that is significantly easier to work with and extremely compact, the **LangChain Expression Language (LCEL)**. This new format relies on a different kind of primitive - a **Runnable** - which is simply an object that wraps a function. Allow dictionaries to be implicitly converted to Runnables and let a **pipe |** operator create a Runnable that passes data from the left to the right (i.e. `fn1 | fn2` is a Runnable), and you have a simple way to specify complex logic!\n",
        "\n",
        "Here are some very representative example Runnables, created via the `RunnableLambda` class:"
      ],
      "metadata": {
        "id": "Qtd3Fjfl3Clo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from functools import partial\n",
        "\n",
        "## Very simple \"take input and return it\"\n",
        "identity = RunnableLambda(lambda x: x)  ## Or RunnablePassthrough works\n",
        "\n",
        "## Given an arbitrary function, you can make a runnable with it\n",
        "def print_and_return(x, preface=\"\"):\n",
        "    print(f\"{preface}{x}\")\n",
        "    return x\n",
        "\n",
        "rprint0 = RunnableLambda(print_and_return)\n",
        "\n",
        "## You can also pre-fill some of values using functools.partial\n",
        "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1: \"))\n",
        "\n",
        "## And you can use the same idea to make your own custom Runnable generator\n",
        "def RPrint(preface=\"\"):\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "\n",
        "## Chaining two runnables\n",
        "chain1 = identity | rprint0\n",
        "chain1.invoke(\"Hello!\")\n",
        "print()\n",
        "\n",
        "## Chaining that one in as well\n",
        "output = (\n",
        "    chain1           ## Prints \"Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        "    | rprint1        ## Prints \"1: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        "    | RPrint(\"2: \")  ## Prints \"2: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        ").invoke(\"Welcome Home!\")\n",
        "\n",
        "## Final Output Is Preserved As \"Welcome Home!\"\n",
        "print(\"\\nOutput:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st7kdho83g1B",
        "outputId": "1ea8d130-db8d-4228-d32d-18a2df7241e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!\n",
            "\n",
            "Welcome Home!\n",
            "1: Welcome Home!\n",
            "2: Welcome Home!\n",
            "\n",
            "Output: Welcome Home!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary Pipelines with Chat Models\n",
        "\n",
        "There's a lot you can do with runnables, but it's important to formalize some best practices. At the moment, it's easiest to use *dictionaries* as our default variable containers for a few key reasons:\n",
        "\n",
        "**Passing dictionaries helps us keep track of our variables by name.**\n",
        "\n",
        "Since dictionaries allow us to propagate named variables (values referenced by keys), using them is great for locking in our chain components' outputs and expectations.\n",
        "\n",
        "**LangChain prompts expect dictionaries of values.**\n",
        "\n",
        "It's quite intuitive to specify an LLM Chain in LCEL to take in a dictionary and produce a string, and equally easy to raise said string back up to be a dictionary. This is very intentional and is partially due to the above reason.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Example 1:** A Simple LLM Chain\n",
        "\n",
        "One of the most fundamental components of classical LangChain is the `LLMChain` that accepts a **prompt** and an **LLM**:\n",
        "\n",
        "- A prompt, usually retrieved from a call like `PromptTemplate.from_template(\"string with {key1} and {key2}\")`, specifies a template for creating a string as output. A dictionary `{\"key1\" : 1, \"key2\" : 2}` could be passed in to get the output `\"string with 1 and 2\"`.\n",
        "    - For chat models like `ChatNVIDIA`, you would use `ChatPromptTemplate.from_messages` instead.\n",
        "- An LLM takes in a string and returns a generated string.\n",
        "    - Chat models like `ChatNVIDIA` work with messages instead, but it's the same idea! Using an **StrOutputParser** at the end will extract the content from the message.\n",
        "\n",
        "The following is a lightweight example of a simple chat chain as described above. All it does is take in an input dictionary and use it fill in a system message to specify the overall meta-objective and a user input to specify query the model."
      ],
      "metadata": {
        "id": "hYGM2Ea38Yhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "## Simple Chat Pipeline\n",
        "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "print(rhyme_chain.invoke({\"input\" : \"Tell me about dogs!\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjjNb_Dh7Qs-",
        "outputId": "3c1a6d21-59f8-44ac-c09d-ca22ea345d85"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dogs are sweet, with tails so neat,\n",
            "They bring joy to our feet, and a trick to repeat.\n",
            "With fur so soft and eyes so bright,\n",
            "They're loving friends, day and night.\n",
            "\n",
            "Their barks are loud, their snuggles tight,\n",
            "They chase their tails with furry delight.\n",
            "From big to small, from old to new,\n",
            "Dogs bring love, and that's what they do!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to just using the code command as-is, we can try using a [**Gradio interface**](https://www.gradio.app/guides/creating-a-chatbot-fast) to play around with our model. Gradio is a popular tool that provides simple building blocks for creating custom generative AI interfaces! The below example shows how you can make an easy gradio chat interface with this particular example chain:"
      ],
      "metadata": {
        "id": "dl36G8cG_YdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "## Streaming Interface\n",
        "\n",
        "def rhyme_chat_stream(message, history):\n",
        "    ## This is a generator function, where each call will yield the next entry\n",
        "    buffer = \"\"\n",
        "    for token in rhyme_chain.stream({\"input\" : message}):\n",
        "        buffer += token\n",
        "        yield buffer\n",
        "\n",
        "## Uncomment when you're ready to try this.\n",
        "demo = gr.ChatInterface(rhyme_chat_stream).queue()\n",
        "window_kwargs = {} # or {\"server_name\": \"0.0.0.0\", \"root_path\": \"/7860/\"}\n",
        "demo.launch(share=True, debug=True, **window_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "e1K2V6B_-5U3",
        "outputId": "3f17489e-1a67-4777-dc07-580e11a4e4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://3cf816ad6c28d8925e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3cf816ad6c28d8925e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}